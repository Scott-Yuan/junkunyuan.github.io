<meta name="description" content="Junkun Yuan&#39;s home page">
<link rel="stylesheet" href="jemdoc.css" type="text/css">
<link rel="shortcut icon" href="resource/citations.jpg">
<title>JunkunYuan's Homepage</title>

<body>
<div id="layout-content" style="margin-top:25px">

<table>
    <tbody>
        <tr>
            <td width="870">
                <div id="toptitle">
                    <h1>Junkun Yuan &nbsp; <img src="resource/junkunyuan_chinese_name.png" height="28px" style="margin-bottom:-5px" alt=''></h1>
                </div>
                <p>
                    <b><FONT size=4>Researcher</FONT></b>
                    <br><br>
                    <a href="https://hunyuan.tencent.com/">Hunyuan Foundation Model Team <img src="resource/hunyuan2.png" width="80px" style="margin-bottom:-5px" alt=''></a>&nbsp;&nbsp;at&nbsp;&nbsp;<a href="https://www.tencent.com/">Tencent <img src="resource/tencent.png" width="100px" style="margin-bottom:-8px" alt=''></a>
                    <br><br>
                    live and work in Shenzhen&#128640, China
                    <br><br>
                    yuanjk0921@outlook.com
                    <br><br><br>
                    <b>&#129303 Try our video generation foundation model at <a href="https://video.hunyuan.tencent.com/">HunyuanVideo</a></b><br><br>
                    <b>&#129303 Try our image generation foundation model at <a href="https://yuanbao.tencent.com/">HunyuanDiT</a></b>
                </p>
            </td>
            <td>
                <img src="resource/citations.jpg" border="0" width="180">
            </td>
        </tr>
    </tbody>
</table>

<style>
.pubs {
    width: 950px;
    /* width: 1100px; */
    padding: 20px 0px;
    display: flex;
    margin-top: 1px;
    margin-bottom: 1px;
    border: none;
    /* border-top-color: solid; */
}

.pubtext {
    /* color: #3E9B57; */
    color: #13A913;
    font-weight: bolder;
    font-size: 13px;
}

.title-normal {
    color: #0E40CD;
    font-size: 15px;
    font-weight:bold;
}

.title-small {
    color: #0E40CD;
    font-size: 14px;
    font-weight:bold;
}

.title-tiny {
    color: #0E40CD;
    font-size: 13px;
    font-weight:bold;
}

.title-tinyy {
    color: #0E40CD;
    font-size: 12px;
    font-weight:bold;
}

.author-pub-normal{
    font-size: 15px;
}

.author-pub-small{
    font-size: 14px;
}

.author-pub-tiny{
    font-size: 13px;
}

.papertext {
    color: #D93053;
    font-weight: bolder;
    font-size: 14px;
}

.abstext {
    color: #666666;
    font-size: 14px;
}

.pub-img-box {
    width: 200px;
    text-align: center;
    margin-right: 20px;
    display: flex;
}

.pub-img {
    margin: auto;
}

.pub-text-box {
    width: 660px;
    display: flex;
}

.pub-text {
    margin: auto 0;
}
</style>

<h2>Biography
    <!-- <small> -->
    <!-- [<a href="https://github.com/junkunyuan"><img src="resource/github.png" width="80px" style="margin-bottom:-5px" alt=''></a>]  -->
    <!-- [<a href="resource/cv.pdf"><img src="resource/cv.jpg" width="160px" style="margin-bottom:-5px" alt=''></a>] -->
    <!-- </small> -->
</h2>
<p>
  I am a researcher of <a href="https://hunyuan.tencent.com/">Hunyuan Foundation Model Team</a>,
  <a href="https://www.tencent.com/">Tencent</a> since 2024.07, working on visual generative foundation models.
  <br><br>

  My research interests include visual & multimodal foundation models and their various downstream applications. 
  <br><br>

  During 2023.09 - 2024.06, I was an intern at <a href="https://hunyuan.tencent.com/">Hunyuan Foundation Model Team</a>,
  <a href="https://www.tencent.com/">Tencent</a>, working with <a href="https://scholar.google.com.hk/citations?user=AjxoEpIAAAAJ&hl=zh-CN&oi=ao"> Wei Liu</a>.
  <br><br>

  During 2022.07 - 2023.08, I was an intern at <a href="http://vis.baidu.com/#/"> Baidu Computer Vision Group</a>, working with <a href="https://zhangxinyu-xyz.github.io/">Xinyu Zhang</a> and <a href="https://jingdongwang2017.github.io/"> Jingdong Wang</a>.
  <br><br>

  I got Ph.D degree from <a href="http://www.zju.edu.cn/">Zhejiang University</a> in 2024.06, supervised by Prof. <a href="https://kunkuang.github.io/">Kun Kuang</a>, Prof. <a href="https://person.zju.edu.cn/0096005">Lanfen Lin</a>, and Prof. 
  <a href="https://person.zju.edu.cn/wufei">Fei Wu</a>.
</p>

<h2>Projects
    <small>
    [<a href="https://github.com/junkunyuan"><img src="resource/github.png" width="80px" style="margin-bottom:-5px" alt=''></a>] 
    <!-- [<a href="resource/cv.pdf"><img src="resource/cv.jpg" width="160px" style="margin-bottom:-5px" alt=''></a>] -->
    </small>
</h2>

<b>[2024.12, HunyuanVideo]</b> &nbsp; 
[<a href="https://arxiv.org/abs/2412.03603">paper</a>] 
[<a href="https://video.hunyuan.tencent.com/">product</a>]  
[<a href="https://github.com/Tencent/HunyuanVideo">project</a>] <img alt="GitHub stars" style="vertical-align:middle" src="https://img.shields.io/github/stars/Tencent/HunyuanVideo?style=social">
<br>
<i class="abstext">
    HunyuanVideo is <b>the first open-sourced large video generation model</b> (with 13B parameters). It is pre-trained on hundreds of millions of hierarchical data with structured captions, <b>outperforming Runway Gen-3, Luma 1.6, and 3 top Chinese video generative models</b>.
</i>
<br><br>

<b>[2024.05, Hunyuan-DiT]</b> &nbsp; 
[<a href="https://arxiv.org/abs/2405.08748">paper</a>] 
[<a href="https://yuanbao.tencent.com/chat/naQivTmsDa">product</a>]  
[<a href="https://github.com/Tencent/HunyuanDiT">project</a>] <img alt="GitHub stars" style="vertical-align:middle" src="https://img.shields.io/github/stars/Tencent/HunyuanDiT?style=social">
<br>
    <i class="abstext">
    Hunyuan-DiT is an <b>open-sourced large image generation model</b> with the structure of diffusion transformer. It is pre-trained on billions of high-quality image data with refined captions of English and Chinese, enabling its understanding abillity of <b>English and Chinese prompts</b>.
    </i>
<br><br>

<b>[2022.02, Awesome-Domain-Generalization]</b> &nbsp; 
[<a href="https://github.com/junkunyuan/Awesome-Domain-Generalization">project</a>] <img alt="GitHub stars" style="vertical-align:middle" src="https://img.shields.io/github/stars/junkunyuan/Awesome-Domain-Generalization?style=social">
<br>
    <i class="abstext">
    I read and organized the relevant researches and resources on <b>visual domain generalization</b> up to 2023.01 (which may be outdated&#x1F605;). You may refer to this repo if you are interested in this topic. And also welcome to contribute to this repo by updating the latest papers.
    </i>
<br><br>

<b>[2024.12, Follow-Your-Emoji (SIGGRAPH-Asia 2024)]</b> &nbsp; 
[<a href="https://arxiv.org/abs/2406.01900">paper</a>] 
[<a href="https://github.com/mayuelala/FollowYourEmoji">project</a>] <img alt="GitHub stars" style="vertical-align:middle" src="https://img.shields.io/github/stars/mayuelala/FollowYourEmoji?style=social">
<br>
    <i class="abstext">
    Follow-Your-Emoji is a diffusion-based <b>portrait animation</b> method. It employs expression-aware landmarks and a facial fine-grained loss to animate a reference portrait by target landmark sequences while preserving portrait identity and temporal consistency and fidelity.
    </i>
<br><br>

<b>[2025.02, Follow-Your-Canvas (AAAI 2025)]</b> &nbsp; 
[<a href="https://arxiv.org/abs/2409.01055">paper</a>] 
[<a href="https://github.com/mayuelala/FollowYourCanvas">project</a>] <img alt="GitHub stars" style="vertical-align:middle" src="https://img.shields.io/github/stars/mayuelala/FollowYourCanvas?style=social">
<br>
    <i class="abstext">
        Follow-Your-Canvas explores <b>higher-resolution video outpainting with extensive content generation</b>. It achieves it by employing spatial window strategy with positional relation learning. It excels in large-scale video outpainting, e.g., from 512 x 512 to 1152 x 2048 (9x).
    </i>
<br><br>

<b>[2023.12, HAP (NeurIPS 2023)]</b> &nbsp; 
[<a href="https://arxiv.org/abs/2310.20695">paper</a>] 
[<a href="https://github.com/junkunyuan/HAP">project</a>] <img alt="GitHub stars" style="vertical-align:middle" src="https://img.shields.io/github/stars/junkunyuan/HAP?style=social">
<br>
    <i class="abstext">
        HAP is the first to introduce masked image modeling as a pre-training framework for <b>human-centric perception</b>. It incorporates human structure priors into pre-training by proposing a pose-guided masking sampling strategy and a human structure-invarint alignment loss.
    </i>
<br><br>

<h2>Publications <small>
    [<a href="https://scholar.google.com.hk/citations?hl=zh-CN&user=j3iFVPsAAAAJ"><img src="resource/google_scholar.png" width="140px" style="margin-bottom:-5px" alt=''></a>] 
    [<a href="https://www.semanticscholar.org/author/Junkun-Yuan/38511927"><img src="resource/semantic_scholar.png" width="140px" style="margin-bottom:-5px" alt=''></a>] 
    [<a href="https://dblp.uni-trier.de/pid/238/0171.html"><img src="resource/dblp.png" width="120px" style="margin-bottom:-15px" alt=''></a>]</small>
</h2>
<br>
<p class="author-pub-normal"> <b>(&#10035 Equal Contribution; &#9993 Corresponding Author.)</b> </p>

<h3 align="center">2025</h3>

<!-- Canvas 2025 AAAI -->
<div class="pubs">
    <div class="pub-img-box">
        <img class="pub-img" src="papers/2025AAAI-canvas/framework.png" width="200px" height="210px"/>
    </div>
    <div class="pub-text-box"><div class="pub-text">
        <p class="title-tiny">Follow-Your-Canvas: Higher-Resolution Video Outpainting with Extensive Content Generation</p>
        <p class="author-pub-normal">Qihua Chen</b><sup>&#10035</sup>, Yue Ma</b><sup>&#10035</sup>, Hongfa Wang</b><sup>&#10035</sup>, Junkun Yuan</b><sup>&#10035</sup><sup>&#9993</sup>, Wenzhe Zhao, Qi Tian, Hongmei Wang, Shaobo Min, Qifeng Chen<sup>&#9993</sup>, Wei Liu</p>
        <b>[2025.02, AAAI]</b>&nbsp;&nbsp;&nbsp;<i class="author-pub-normal">AAAI Conference on Artificial Intelligence</i> <br><br>
        <i class="abstext">
            <b>(conditional video generation)</b> It explores higher-resolution video outpainting with extensive content generation. It achieves it by employing spatial window strategy with positional relation learning. It excels in large-scale video outpainting, e.g., from 512 x 512 to 1152 x 2048 (9x).
        </i>
        <br><br>
        [<a href="https://arxiv.org/abs/2409.01055"><span class="papertext">arXiv</span></a>]
        [<a href="https://follow-your-canvas.github.io/"><span class="papertext">Project</span></a>]
        [<a href="https://github.com/mayuelala/FollowYourCanvas"><span class="papertext">Code</span></a>]

    </div></div>
</div>

<h3 align="center">2024</h3>

<!-- HunyuanVideo 2024 technical report -->
<div class="pubs">
    <div class="pub-img-box">
        <img class="pub-img" src="papers/2024arXiv-hunyuanvideo/framework.png" width="200px" height="160px"/>
    </div>
    <div class="pub-text-box"><div class="pub-text">
        <p class="title-normal">HunyuanVideo: A Systematic Framework For Large Video Generative Models
        </p>
        <p class="author-pub-normal">Hunyuan Foundation Model Team</p>
        <b>[2024.12, Technical Report]</b>&nbsp;&nbsp;&nbsp;<i class="author-pub-normal"></i> <br><br>
        <i class="abstext">
            <b>(video generation foundation model)</b> It presents HunyuanVideo as a foundation model, which outperforms Runway Gen-3, Luma 1.6, and 3 top performing Chinese video generative models.
        </i>
        <br><br>
        [<a href="https://arxiv.org/abs/2412.03603"><span class="papertext">arXiv</span></a>]
        [<a href="https://video.hunyuan.tencent.com/"><span class="papertext">Try it</span></a>]
        [<a href="https://github.com/Tencent/HunyuanVideo"><span class="papertext">Code</span></a>]
    </div></div>
</div>

<!-- Emoji 2024 SigGraphAisa -->
<div class="pubs">
    <div class="pub-img-box">
        <img class="pub-img" src="papers/2024SiggraphAsia-Emoji/framework.png" width="200px" height="220px"/>
    </div>
    <div class="pub-text-box"><div class="pub-text">
        <p class="title-normal">Follow-Your-Emoji: Fine-Controllable and Expressive Freestyle Portrait Animation</p>
        <p class="author-pub-normal">Yue Ma</b><sup>&#10035</sup>, Hongyu Liu</b><sup>&#10035</sup>, Hongfa Wang</b><sup>&#10035</sup>, Heng Pan</b><sup>&#10035</sup>, Yingqing He, <b>Junkun Yuan</b>, Ailing Zeng, Chengfei Cai, Heung-Yeung Shum, Wei Liu<sup>&#9993</sup>, Qifeng Chen<sup>&#9993</sup></p>
        <b>[2024.12, SIGGRAPH-Asia]</b>&nbsp;&nbsp;&nbsp;<i class="author-pub-normal">Computer Graphics and Interactive Techniques-Asia</i> <br><br>
        <i class="abstext">
            <b>(conditional video generation)</b> An interesting diffusion-based framework using expression-aware landmarks and a facial fine-grained loss for portrait animation, which animates a reference portrait by target landmark sequences while preserving portrait identity and temporal consistency and fidelity.
        </i>
        <br><br>
        [<a href="https://arxiv.org/abs/2406.01900"><span class="papertext">arXiv</span></a>]
        [<a href="papers/2024SiggraphAsia-Emoji/paper.pdf"><span class="papertext">PDF</span></a>]
        [<a href="https://follow-your-emoji.github.io/"><span class="papertext">Project</span></a>]
        [<a href="https://github.com/mayuelala/FollowYourEmoji"><span class="papertext">Code</span></a>]
    </div></div>
</div>

<!-- prompt 2024 IJCV -->
<div class="pubs">
    <div class="pub-img-box">
        <img class="pub-img" src="papers/2024IJCV-prompt/framework.png" width="200px" height="190px"/>
    </div>
    <div class="pub-text-box"><div class="pub-text">
        <p class="title-normal">Mutual Prompt Leaning for Vision Language Models</p>
        <p class="author-pub-normal">Sifan Long</b><sup>&#10035</sup>, Zhen Zhao</b><sup>&#10035</sup>, Junkun Yuan</b><sup>&#10035</sup>, Zichang Tan</b><sup>&#10035</sup>, Jiangjiang Liu, Jingyuan Feng, Shengsheng Wang<sup>&#9993</sup>, Jingdong Wang</p>
        <b>[2024.09, IJCV]</b>&nbsp;&nbsp;&nbsp;<i class="author-pub-normal">International Journal of Computer Vision</i> <br><br>
        <i class="abstext">
            <b>(prompt learning)</b> It presents fine-grained text prompt to decompose image features into finer-grained semantics, and text-reorganized visual prompt to attend to class-related representations.
        </i>
        <br><br>
        [<a href="papers/2024IJCV-prompt/paper.pdf"><span class="papertext">PDF</span></a>]
    </div></div>
</div>

<!-- KDD 2024 NPT -->
<div class="pubs">
    <div class="pub-img-box">
        <img class="pub-img" src="papers/2024KDD-NPT/framework.png" width="200px" height="190px"/>
    </div>
    <div class="pub-text-box"><div class="pub-text">
        <p class="title-normal">Neural Collapse Anchored Prompt Tuning for Generalizable Vision-Language Models</p>
        <p class="author-pub-normal">Didi Zhu, Zexi Li, Min Zhang, <b>Junkun Yuan</b>, Jiashuo Liu, Kun Kuang<sup>&#9993</sup>, Chao Wu</p>
        <b>[2024.08, KDD]</b>&nbsp;&nbsp;&nbsp;<i class="author-pub-normal">ACM SIGKDD Conference on Knowledge Discovery and Data Mining</i> <br><br>
        <i class="abstext">
            <b>(prompt learning)</b> It makes the first attempt to borrow the idea of neural collapes for improving the representations of multimodal models by means of prompt learning. It lets multimodal representations exhibit a equiangular tight frame structure to improve the generalization performance of the model.
        </i>
        <br><br>
        [<a href="https://arxiv.org/abs/2306.15955"><span class="papertext">arXiv</span></a>]
    </div></div>
</div>

<!-- HunyuanDit 2024 technical report -->
<div class="pubs">
    <div class="pub-img-box">
        <img class="pub-img" src="papers/2024arXiv-hunyuandit/framework.png" width="200px" height="180px"/>
    </div>
    <div class="pub-text-box"><div class="pub-text">
        <p class="title-normal">Hunyuan-DiT: A Powerful Multi-Resolution Diffusion Transformer with Fine-Grained Chinese Understanding
        </p>
        <p class="author-pub-normal">Hunyuan Foundation Model Team</p>
        <b>[2024.05, Technical Report]</b>&nbsp;&nbsp;&nbsp;<i class="author-pub-normal"></i> <br><br>
        <i class="abstext">
            <b>(image generation foundation model)</b> It presents Hunyuan-DiT as a foundation model, which is a text-to-image diffusion transformer with fine-grained understanding of both English and Chinese.
        </i>
        <br><br>
        [<a href="https://arxiv.org/abs/2405.08748"><span class="papertext">arXiv</span></a>]
        [<a href="https://yuanbao.tencent.com/chat/naQivTmsDa"><span class="papertext">Try it</span></a>]
        [<a href="https://github.com/Tencent/HunyuanDiT"><span class="papertext">Code</span></a>]
    </div></div>
</div>

<!-- ICASSP 2024 Domaindiff -->
<div class="pubs">
    <div class="pub-img-box">
        <img class="pub-img" src="papers/2024ICASSP-Domaindiff/framework.png" width="200px" height="150px"/>
    </div>
    <div class="pub-text-box"><div class="pub-text">
        <p class="title-normal">Domaindiff: Boost out-of-Distribution Generalization with Synthetic Data</p>
        <p class="author-pub-normal">Qiaowei Miao, <b>Junkun Yuan</b>, Shengyu Zhang, Fei Wu, Kun Kuang<sup>&#9993</sup></p>
        <b>[2024.04, ICASSP]</b>&nbsp;&nbsp;&nbsp;<i class="author-pub-normal">International Conference on Acoustics, Speech, and Signal Processing</i> <br><br>
        <i class="abstext">
            It employs diffusion models to synthesize data for improving OOD generalization performance.
        </i>
        <br><br>
        [<a href="papers/2024ICASSP-Domaindiff/paper.pdf"><span class="papertext">PDF</span></a>]
    </div></div>
</div>

<h3 align="center">2023</h3>

<!-- NeurIPS 2023 HAP -->
<div class="pubs">
    <div class="pub-img-box">
        <img class="pub-img" src="papers/2023NeurIPS-HAP/framework.png" width="200px" height="210px"/>
    </div>
    <div class="pub-text-box"><div class="pub-text">
        <p class="title-normal">HAP: Structure-Aware Masked Image Modeling for Human-Centric Perception</p>
        <p class="author-pub-small"><b>Junkun Yuan</b><sup>&#10035</sup>, Xinyu Zhang<sup>&#10035</sup><sup>&#9993</sup>, Hao Zhou, Jian Wang, Zhongwei Qiu, Zhiyin Shao, Shaofeng Zhang, Sifan Long, Kun Kuang<sup>&#9993</sup>, Kun Yao, Junyu Han, Errui Ding, Lanfen Lin, Fei Wu, Jingdong Wang<sup>&#9993</sup></p>
        <b>[2023.12, NeurIPS]</b>&nbsp;&nbsp;&nbsp;<i class="author-pub-normal"> Advances in Neural Information Processing Systems</i> <br><br>
        <i class="abstext">
            <b>(visual self-supervised learning)</b> It first introduces masked image modeling as a pre-training framework for human-centric perception. It incorporates human structure priors into pre-training by proposing a pose-guided masking sampling strategy and a human structure-invarint alignment loss.
        </i>
        <br>
        <br>
        [<a href="https://arxiv.org/abs/2310.20695"><span class="papertext">arXiv</span></a>]
        [<a href="papers/2023NeurIPS-HAP/paper.pdf"><span class="papertext">PDF</span></a>]
        [<a href="papers/2023NeurIPS-HAP/poster.pdf"><span class="papertext">Poster</span></a>]
        [<a href="https://zhangxinyu-xyz.github.io/hap.github.io/"><span class="papertext">Project</span></a>]
        [<a href="https://github.com/junkunyuan/HAP"><span class="papertext">Code</span></a>]
    </div></div>
</div>

<!-- TKDE 2023 CSAC -->
<div class="pubs">
    <div class="pub-img-box">
        <img class="pub-img" src="papers/2023TKDE-CSAC/framework.jpg" width="200px" height="175px"/>
    </div>
    <div class="pub-text-box"><div class="pub-text">
        <p class="title-small">Collaborative Semantic Aggregation and Calibration for Federated Domain Generalization</p>
        <p class="author-pub-normal"><b>Junkun Yuan</b><sup>&#10035</sup>, Xu Ma<sup>&#10035</sup>, Defang Chen, Fei Wu, Lanfen Lin, Kun Kuang<sup>&#9993</sup></p>
        <b>[2023.12, TKDE]</b>&nbsp;&nbsp;&nbsp;<i class="author-pub-normal"> IEEE Transactions on Knowledge and Data Engineering</i> <br><br>
        <i class="abstext">
            <b>(out-of-distribution generalization)</b> To protect data privacy, it solves the federated domain generalization task by proposing a collaborative semantic aggregation and calibration method with local semantic acquisition, data-free semantic aggregation, and cross-layer semantic calibration.
        </i>
        <br>
        <br>
        [<a href="https://arxiv.org/abs/2110.06736"><span class="papertext">arXiv</span></a>]
        [<a href="papers/2023TKDE-CSAC/paper.pdf"><span class="papertext">PDF</span></a>]
        [<a href="https://github.com/junkunyuan/CSAC"><span class="papertext">Code</span></a>]
    </div></div>
</div>

<!-- ICCV 2023 MAP -->
<div class="pubs">
    <div class="pub-img-box">
        <img class="pub-img" src="papers/2023ICCV-MAP/framework.jpg" width="200px" height="200px"/>
    </div>
    <div class="pub-text-box"><div class="pub-text">
        <p class="title-small">MAP: Towards Balanced Generalization of IID and OOD through Model-Agnostic Adapters</p>
        <p class="author-pub-normal">Min Zhang, <b>Junkun Yuan</b>, Yue He, Wenbin Li, Zhengyu Chen, Kun Kuang<sup>&#9993</sup></p>
        <b>[2023.10, ICCV]</b>&nbsp;&nbsp;&nbsp;<i class="author-pub-normal">International Conference on Computer Vision</i> <br><br>
        <i class="abstext">
            <b>(out-of-distribution generalization)</b> It presents empirical evidence that existing OOD methods exhibit subpar performance when faced with minor distribution shifts, and proposes a bilevel optimization strategy, leveraging adapters to strike a balance between IID and OOD generalization.
        </i>
        <br><br>
        [<a href="papers/2023ICCV-MAP/paper.pdf"><span class="papertext">PDF</span></a>]
        [<a href="https://github.com/remiMZ/MAP-ICCV23"><span class="papertext">Code</span></a>]
    </div></div>
</div>

<!-- ICCV 2023 CAM -->
<div class="pubs">
    <div class="pub-img-box">
        <img class="pub-img" src="papers/2023ICCV-CAM/framework.jpg" width="200px" height="160px"/>
    </div>
    <div class="pub-text-box"><div class="pub-text">
        <p class="title-normal">Universal Domain Adaptation via Compressive Attention Matching</p>
        <p class="author-pub-normal">Didi Zhu<sup>&#10035</sup>, Yinchuan Li<sup>&#10035</sup>, <b>Junkun Yuan</b>, Zexi Li, Kun Kuang<sup>&#9993</sup>, Chao Wu<sup>&#9993</sup></p>
        <b>[2023.10, ICCV]</b>&nbsp;&nbsp;&nbsp;<i class="author-pub-normal">International Conference on Computer Vision</i> <br><br>
        <i class="abstext">
            <b>(domain adaptation)</b> To better align common features and separate target classes for universal domain adaptation, it makes attempts of leveraging attention maps in ViTs to perform prediction. 
        </i>
        <br><br>
        [<a href="https://arxiv.org/abs/2304.11862"><span class="papertext">arXiv</span></a>]
        [<a href="papers/2023ICCV-CAM/paper.pdf"><span class="papertext">PDF</span></a>]
    </div></div>
</div>

<!-- ICCV 2023 CTP-TFT -->
<div class="pubs">
    <div class="pub-img-box">
        <img class="pub-img" src="papers/2023ICCV-CTP-TFT/framework.png" width="200px" height="220px"/>
    </div>
    <div class="pub-text-box"><div class="pub-text">
        <p class="title-normal">Task-Oriented Multi-Modal Mutual Leaning for Vision-Language Models</p>
        <p class="author-pub-normal">Sifan Long<sup>&#10035</sup>, Zhen Zhao<sup>&#10035</sup>, <b>Junkun Yuan</b><sup>&#10035</sup>, Zichang Tan, Jiangjiang Liu, Luping Zhou, Shengsheng Wang<sup>&#9993</sup>, Jingdong Wang<sup>&#9993</sup></p>
        <b>[2023.10, ICCV]</b>&nbsp;&nbsp;&nbsp;<i class="author-pub-normal">International Conference on Computer Vision</i> <br><br>
        <i class="abstext">
            <b>(prompt learning)</b> It presents a class-aware text prompt to enrich generated prompts with label-related image information, and makes the image branch attend to class-related representations. The text and image branches mutually promote to enhance the adaptation of vision-language models. 
        </i>
        <br><br>
        [<a href="https://arxiv.org/abs/2303.17169"><span class="papertext">arXiv</span></a>]
        [<a href="papers/2023ICCV-CTP-TFT/paper.pdf"><span class="papertext">PDF</span></a>]
    </div></div>
</div>

<!-- TMLR 2023 CAEv2 -->
<div class="pubs">
    <div class="pub-img-box">
        <img class="pub-img" src="papers/2023TMLR-CAEv2/framework.png" width="200px" height="200px"/>
    </div>
    <div class="pub-text-box"><div class="pub-text">
        <p class="title-normal">CAE v2: Context Autoencoder with CLIP Latent Alignment</p>
        <p class="author-pub-normal">Xinyu Zhang<sup>&#10035</sup>, Jiahui Chen<sup>&#10035</sup>, <b>Junkun Yuan</b>, Qiang Chen, Jian Wang, Xiaodi Wang, Shumin Han, Xiaokang Chen, Jimin Pi, Kun Yao, Junyu Han, Errui Ding, Jingdong Wang<sup>&#9993</sup></p>
        <b>[2023.09, TMLR]</b>&nbsp;&nbsp;&nbsp;<i class="author-pub-normal"> Transactions on Machine Learning Research</i> <br><br>
        <i class="abstext">
            <b>(visual self-supervised learning)</b> It proposes CAE v2, a new CLIP-guided masked image modeling method, which (i) uses both masked and visible tokens as the distillation target, and (ii) employs a masking ratio proportional to the model size, achieving superior performance than existing SOTAs.
        </i>
        <br><br>
        [<a href="https://arxiv.org/abs/2211.09799"><span class="papertext">arXiv</span></a>]
        [<a href="papers/2023TMLR-CAEv2/paper.pdf"><span class="papertext">PDF</span></a>]
        [<a href="https://github.com/Atten4Vis/CAE"><span class="papertext">Code</span></a>]
    </div></div>
</div>

<!-- KDD 2023 HeterogeneityDG -->
<div class="pubs">
    <span class="pub-img-box">
        <img class="pub-img" src="papers/2023KDD-HeterogeneityDG/framework.png" width="200px" height="160px" alt="" />
    </span>
    <div class="pub-text-box"><div class="pub-text">
        <p class="title-tiny">Quantitatively Measuring and Contrastively Exploring Heterogeneity for Domain Generalization</p>
        <p class="author-pub-normal">Yunze Tong, <b>Junkun Yuan</b>, Min Zhang, Didi Zhu, Keli Zhang, Fei Wu, Kun Kuang<sup>&#9993</sup></p>
        <b>[2023.08, KDD]</b>&nbsp;&nbsp;&nbsp;<i class="author-pub-normal">ACM SIGKDD Conference on Knowledge Discovery and Data Mining </i> <br><br>
        <i class="abstext">
            <b>(out-of-distribution generalization)</b> It tackles the label heterogeneity problem, i.e., the ground-truth labels could be sub-optimal, and proposes a method to quantitatively measure and address it.
        </i>
        <br><br>
        [<a href="https://arxiv.org/abs/2305.15889"><span class="papertext">arXiv</span></a>]
        [<a href="papers/2023KDD-HeterogeneityDG/paper.pdf"><span class="papertext">PDF</span></a>]
        [<a href="https://github.com/YunzeTong/HTCL"><span class="papertext">Code</span></a>]
    </div></div>
</div>

<!-- TKDD 2023 IV-DG -->
<div class="pubs">
    <span class="pub-img-box">
        <img class="pub-img" src="papers/2023TKDD-iv-dg/framework.jpg" width="200px" height="200px" alt="" />
    </span>
    <div class="pub-text-box"><div class="pub-text">
        <p class="title-normal">Instrumental Variable-Driven Domain Generalization with Unobserved Confounders</p>
        <p class="author-pub-small"><b>Junkun Yuan</b>, Xu Ma, Ruoxuan Xiong, Mingming Gong, Xiangyu Liu, Fei Wu, Lanfen Lin, Kun Kuang<sup>&#9993</sup></p>
        <b>[2023.06, TKDD]</b>&nbsp;&nbsp;&nbsp;<i class="author-pub-normal"> ACM Transactions on Knowledge Discovery from Data</i><br><br>
        <i class="abstext">
            <b>(out-of-distribution generalization, causal learning)</b> It provides a causal view on domain generalization that separates domain-invariant and domain-specific parts of data. It then proposes to learn the domain-invariant relationship between the input features and the labels through instrumental variable method. It allows one to remove unobserved confounders and obtain a generalizable model.
        </i>
        <br><br>
        [<a href="https://arxiv.org/abs/2110.01438"><span class="papertext">arXiv</span></a>] 
        [<a href="papers/2023TKDD-iv-dg/slides.pdf"><span class="papertext">Slides</span></a>] 
    </div></div>
</div>

<!-- TMM 2023 KDDRL -->
<div class="pubs">
    <div class="pub-img-box">
        <img class="pub-img" src="papers/2023TMM-KDDRL/framework.png" width="200px" height="160px"/>
    </div>
    <div class="pub-text-box"><div class="pub-text">
        <p class="title-tinyy">Knowledge Distillation-based Domain-invariant Representation Learning for Domain Generalization</p>
        <p class="author-pub-small">Ziwei Niu, <b>Junkun Yuan</b>, Xu Ma, Yingying Xu, Jing Liu, Yen-Wei Chen, Ruofeng Tong, Lanfen Lin<sup>&#9993</sup></p>
        <b>[2023.04, TMM]</b>&nbsp;&nbsp;&nbsp;<i class="author-pub-normal">IEEE Transactions on Multimedia</i> <br><br>
        <i class="abstext">
            <b>(out-of-distribution generalization)</b> Inspired by knowledge distillation, it performs two-stage distillation with (i) multiple students: each is trained on one dataset, and (ii) one leader student. 
        </i>
        <br><br>
        [<a href="papers/2023TMM-KDDRL/paper.pdf"><span class="papertext">PDF</span></a>]
    </div></div>
</div>


<h3 align="center">2022</h3>

<!-- IJCV 2022 DSBF -->
<div class="pubs">
    <div class="pub-img-box">
        <img class="pub-img" src="papers/2022IJCV-DSBF/framework.jpg" width="200px" height="160px"/>
    </div>
    <div class="pub-text-box"><div class="pub-text">
        <p class="title-normal">Domain-Specific Bias Filtering for Single Labeled Domain Generalization</p>
        <p class="author-pub-normal"><b>Junkun Yuan</b><sup>&#10035</sup>, Xu Ma<sup>&#10035</sup>, Defang Chen, Kun Kuang<sup>&#9993</sup>, Fei Wu, Lanfen Lin</p>
        <b>[2022.11, IJCV]</b>&nbsp;&nbsp;&nbsp;<i class="author-pub-normal">International Journal of Computer Vision</i> <br><br>
        <i class="abstext">
            <b>(out-of-distribution generalization)</b> It proposes a task called single labeled domain generalization where only one source dataset is labeled, as well as a method named domain-specific bias filtering.
        </i>
        <br><br>
        [<a href="https://arxiv.org/abs/2110.00726"><span class="papertext">arXiv</span></a>]
        [<a href="papers/2022IJCV-DSBF/paper.pdf"><span class="papertext">PDF</span></a>]
        [<a href="https://github.com/junkunyuan/DSBF"><span class="papertext">Code</span></a>]
    </div></div>
</div>

<!-- MM 2022 CEG -->
<div class="pubs">
    <span class="pub-img-box">
        <img class="pub-img" src="papers/2022MM-CEG/framework.jpg" width="200px" height="180px"/>
    </span>
    <div class="pub-text-box"><div class="pub-text">
        <p class="title-small">Label-Efficient Domain Generalization via Collaborative Exploration and Generalization</p>
        <p class="author-pub-normal"><b>Junkun Yuan</b><sup>&#10035</sup>, Xu Ma<sup>&#10035</sup>, Defang Chen, Kun Kuang<sup>&#9993</sup>, Fei Wu, Lanfen Lin</p>
        <b>[2022.10, MM]</b>&nbsp;&nbsp;&nbsp;<i class="author-pub-normal">International Conference on Multimedia</i> <br><br>
        <i class="abstext">
            <b>(out-of-distribution generalization)</b> To enable generalization learning with limited annotation, it proposes a framework called collaborative exploration and generalization, which jointly optimizes active exploration and semi-supervised generalization for the label-efficient domain generalization task.
        </i>
        <br><br>
        [<a href="https://arxiv.org/abs/2208.03644"><span class="papertext">arXiv</span></a>]
        [<a href="papers/2022MM-CEG/paper.pdf"><span class="papertext">PDF</span></a>]
        [<a href="https://github.com/junkunyuan/CEG"><span class="papertext">Code</span></a>]
    </div></div>
</div>

<!-- ArXiv 2022 CCM -->
<div class="pubs">
    <div class="pub-img-box">
        <img class="pub-img" src="papers/2022arXiv-CCM/framework.jpg" width="200px" height="160px"/>
    </div>
    <div class="pub-text-box"><div class="pub-text">
        <p class="title-normal">Domain Generalization via Contrastive Causal Learning</p>
        <p class="author-pub-normal">Qiaowei Miao, <b>Junkun Yuan</b>, Kun Kuang</p>
        <b>[2022.10, Technical Report]</b>&nbsp;&nbsp;&nbsp;<i class="author-pub-normal"></i> <br><br>
        <i class="abstext">
            <b>(out-of-distribution generalization, causal learning)</b> It proposes a generalizable causal model by controlling the unstable domain factor and quantifying the causal effects with front-door criterion.
        </i>
        <br><br>
        [<a href="https://arxiv.org/abs/2210.02655"><span class="papertext">arXiv</span></a>]
        [<a href="https://github.com/MiaoQiaowei/CCM"><span class="papertext">Code</span></a>]
    </div></div>
</div>

<!-- Neurcomputing 2022 ACDA -->
<div class="pubs">
    <span class="pub-img-box">
        <img class="pub-img" src="papers/2022Neurocomputing-ACDA/framework.jpg" width="200px" height="185px" alt="" />
    </span>
    <div class="pub-text-box"><div class="pub-text">
        <p class="title-small">Attention-based Cross-Layer Domain Alignment for Unsupervised Domain Adaptation</p>
        <p class="author-pub-normal">Xu Ma, <b>Junkun Yuan</b>, Yen-wei Chen, Ruofeng Tong, Lanfen Lin<sup>&#9993</sup></p>
        <b>[2022.08, Neurocomputing]</b>&nbsp;&nbsp;&nbsp;<i class="author-pub-normal"></i> 
        <br><br>
        <i class="abstext">
            <b>(domain adaptation)</b> 
            It proposes attention-based cross-layer domain alignment, which captures the semantic relationship between the source domain and the target domain across model layers and calibrates each level of semantic information automatically through a dynamic attention mechanism.
        </i>
        <br><br>
        [<a href="https://arxiv.org/abs/2202.13310"><span class="papertext">arXiv</span></a>]
        [<a href="papers/2022Neurocomputing-ACDA/paper.pdf"><span class="papertext">PDF</span></a>]
        [<a href="https://github.com/MaXuSun/ACDA"><span class="papertext">Code</span></a>]
    </div></div>
</div>

<!-- TKDE 2022 DeR-CFR -->
<div class="pubs">
    <span class="pub-img-box">
        <img class="pub-img" src="papers/2022TKDE-DeR-CFR/framework.jpg" width="200px" height="170px"/>
    </span>
    <div class="pub-text-box"><div class="pub-text">
        <p class="title-normal">Learning Decomposed Representations for Treatment Effect Estimation</p>
        <p class="author-pub-tiny">Anpeng Wu<sup>&#10035</sup>, <b>Junkun Yuan</b><sup>&#10035</sup>, Kun Kuang<sup>&#9993</sup>, Bo Li, Pan Zhou, Jianrong Tao, Qiang Zhu, Yueting Zhuang, Fei Wu</p>
        <b>[2022.02, TKDE]</b>&nbsp;&nbsp;&nbsp;<i class="author-pub-normal">IEEE Transactions on Knowledge and Data Engineering</i> <br><br>
        <i class="abstext">
            <b>(causal reasoning)</b> It separates confounders by learning representations of confounders and non-confounders, balancing confounders with sample re-weighting, and estimating treatment effect.
        </i>
        <br><br>
        [<a href="https://arxiv.org/abs/2006.07040"><span class="papertext">arXiv</span></a>]
        [<a href="papers/2022TKDE-DeR-CFR/paper.pdf"><span class="papertext">PDF</span></a>]
        [<a href="https://github.com/anpwu/DeR-CFR"><span class="papertext">Code</span></a>]
    </div></div>
</div>

<!-- TKDD 2022 AutoIV -->
<div class="pubs">
    <span class="pub-img-box">
        <img class="pub-img" src="papers/2022TKDD-AutoIV/framework.png" width="200px" height="185px" alt="" />
    </span>
    <div class="pub-text-box"><div class="pub-text">
        <p class="title-small">Auto IV: Counterfactual Prediction via Automatic Instrumental Variable Decomposition</p>
        <p class="author-pub-normal"><b>Junkun Yuan</b><sup>&#10035</sup>, Anpeng Wu<sup>&#10035</sup>, Kun Kuang<sup>&#9993</sup>, Bo Li, Runze Wu, Fei Wu, Lanfen Lin</p>
        <b>[2022.01, TKDD]</b>&nbsp;&nbsp;&nbsp;<i class="author-pub-normal">ACM Transactions on Knowledge Discovery from Data </i> 
        <br><br>
        <i class="abstext">
            <b>(causal reasoning)</b> 
            Instrumental Variable (IV) is a powerful tool for causal inference, but it is hard to pre-define valid IVs. 
            It proposes an automatic instrumental variable decomposition algorithm to generate effective IV representations from observed variables for IV-based counterfactual prediction.
        </i>
        <br><br>
        [<a href="https://arxiv.org/abs/2107.05884"><span class="papertext">arXiv</span></a>]
        [<a href="papers/2022TKDD-AutoIV/paper.pdf"><span class="papertext">PDF</span></a>]
        [<a href="https://github.com/junkunyuan/AutoIV"><span class="papertext">Code</span></a>]
    </div></div>
</div>

<h3 align="center">2021 and before
</h3>

<!-- TKDE 2021 SGN -->
<div class="pubs">
    <div class="pub-img-box">
        <img class="pub-img" src="papers/2021TKDE-SGN/framwork.jpg" width="200px" height="175px"/>
    </div>
    <div class="pub-text-box"><div class="pub-text">
        <p class="title-normal">Subgraph Networks with Application to Structural Feature Space Expansion</p>
        <p class="author-pub-tiny">Qi Xuan<sup>&#9993</sup>, Jinhuan Wang, Minghao Zhao, <b>Junkun Yuan</b>, Chenbo Fu, Zhongyuan Ruan<sup>&#9993</sup>, Guanrong Chen</p>
        <b>[2021.12, TKDE]</b>&nbsp;&nbsp;&nbsp;<i class="author-pub-normal">IEEE Transactions on Knowledge and Data Engineering</i> 
        <br><br>
        <i class="abstext">
            <b>(network embedding)</b> It introduces a new concept of subgraph network and constructs 1st/2nd-order SGNs. SGNs provide additional
             structural features to the features that extracted by advanced network-embedding methods, largely improving model performance on the network classification task.
        </i>
        <br><br>
        [<a href="https://arxiv.org/abs/1903.09022"><span class="papertext">arXiv</span></a>] 
        [<a href="papers/2021TKDE-SGN/paper.pdf"><span class="papertext">PDF</span></a>]
        [<a href="https://github.com/GalateaWang/SGNs-master"><span class="papertext">Code</span></a>]
    </div></div>
</div>

<!-- ECAI 2020 GAPGAN -->
<div class="pubs">
    <div class="pub-img-box">
        <img class="pub-img" src="papers/2020ECAI-GAPGAN/framework.jpg" width="200px" height="175px">
    </div>
    <div class="pub-text-box"><div class="pub-text">
        <p class="title-tinyy">Black-box Adversarial Attacks Against Deep Learning Based Malware Binaries Detection with GAN</p>
        <p class="author-pub-normal"><b>Junkun Yuan</b>, Shaofang Zhou, Lanfen Lin<sup>&#9993</sup>, Feng Wang, Jia Cui</p>
        <b>[2020.08, ECAI]</b>&nbsp;&nbsp;&nbsp;<i class="author-pub-normal">European Conference on Artificial Intelligence</i> 
        <br><br>
        <i class="abstext">
            <b>(adversarial attack)</b> To attack malware detector, it proposes a black-box attack framework called GAPGAN to generate adversarial payloads via GAN. It generates malicious payloads and appends them to the malware to craft "benign binaries" while preserving its original attacking functionality.
        </i>
        <br><br>
        [<a href="papers/2020ECAI-GAPGAN/paper.pdf"><span class="papertext">PDF</span></a>]
        [<a href="papers/2020ECAI-GAPGAN/slides.pdf"><span class="papertext">Slides</span></a>]
    </div></div>
</div>

<!-- ISI 2019 CNN -->
<div class="pubs">
    <div class="pub-img-box">
        <img class="pub-img" src="papers/2019ISI-DGA/framework.jpg" width="200px" height="160px">
    </div>
    <div class="pub-text-box"><div class="pub-text">
        <p class="title-normal">CNN-based DGA Detection with High Coverage</p>
        <p class="author-pub-normal">Shaofang Zhou, Lanfen Lin<sup>&#9993</sup>, <b>Junkun Yuan</b>, Feng Wang, Zhaoting Ling, Jia Cui</p>
        <b>[2019.07, ISI]</b>&nbsp;&nbsp;&nbsp;<i class="author-pub-normal">International Conference on Intelligence and Security Informatics</i> <br><br>
        <i class="abstext">
            <b>(adversarial attack)</b> Attackers use algorithms to create domains for attacks. It presents a temporal convolutional network-based generated domain detection method with high accuracy and coverage.
        </i>
        <br><br>
        [<a href="papers/2019ISI-DGA/paper.pdf"><span class="papertext">PDF</span></a>] 
    </div></div>
</div>

<!--
<h2>Professional Services</h2>
<span>Conference reviewer: AAAI 2023, MM 2023, ICCV 2023.</span>
<br><br>
<span>Journal reviewer: TPAMI, TNNLS, NN, TCSVT.</span>
-->
<!-- <br><br><br> -->
<!-- <span>Help to review for conferences: ICML 2021; KDD 2021; ICCV 2021, 2023; CIKM 2021; NeurIPS 2021; ICDM 2022; AAAI 2021, 2022; ICLR 2023; CVPR 2021, 2022.</span> -->
<br><br>

<div id="footer">
    <div id="footer-text"></div>
</div>

</div>

Last updated on Dec. 31, 2024.
</body>
