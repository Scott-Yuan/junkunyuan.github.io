<meta name="description" content="Junkun Yuan&#39;s home page">
<link rel="stylesheet" href="jemdoc.css" type="text/css">
<link rel="shortcut icon" href="resource/zju.png">
<title>JunkunYuan's Homepage</title>

<body>
<div id="layout-content" style="margin-top:25px">

<table>
    <tbody>
        <tr>
            <td width="870">
                <div id="toptitle">
                    <h1>Junkun Yuan &nbsp; <img src="resource/junkunyuan_chinese_name.png" height="28px" style="margin-bottom:-5px" alt=''></h1>
                </div>
                <p>
                    <br>
                    <b><FONT size=4>Ph.D. Student</FONT></b>
                    <br><br>
                    Institute of Artificial Intelligence<br>
                    College of Computer Science and Technology<br>
                    Zhejiang University<br>
                    <br>
                    <!-- Address: No. 38 Zheda Road, Xihu District, Hangzhou, Zhejiang 310027, P.R.China -->
                    <!-- <br><br> -->
                    E-mail: yuanjk at zju dot edu dot cn
                </p>
            </td>
            <td>
                <img src="resource/blue2.jpg" border="0" width="220">
            </td>
        </tr>
    </tbody>
</table>

<style>
.pubs {
    width: 950px;
    /* width: 1100px; */
    padding: 20px 0px;
    display: flex;
    margin-top: 1px;
    margin-bottom: 1px;
    border: none;
    /* border-top-color: solid; */
}

.pubtext {
    /* color: #3E9B57; */
    color: #13A913;
    font-weight: bolder;
    font-size: 13px;
}

.title-normal {
    color: #0E40CD;
    font-size: 15px;
    font-weight:bold;
}

.title-small {
    color: #0E40CD;
    font-size: 14px;
    font-weight:bold;
}

.title-tiny {
    color: #0E40CD;
    font-size: 13px;
    font-weight:bold;
}

.title-tinyy {
    color: #0E40CD;
    font-size: 12px;
    font-weight:bold;
}

.author-pub-normal{
    font-size: 15px;
}

.author-pub-small{
    font-size: 14px;
}

.author-pub-tiny{
    font-size: 13px;
}

.papertext {
    color: #D93053;
    font-weight: bolder;
    font-size: 14px;
}

.abstext {
    color: #666666;
    font-size: 14px;
}

.pub-img-box {
    width: 200px;
    text-align: center;
    margin-right: 20px;
    display: flex;
}

.pub-img {
    margin: auto;
}

.pub-text-box {
    width: 660px;
    display: flex;
}

.pub-text {
    margin: auto 0;
}
</style>

<h2>Biography
    <small>
    [<a href="https://github.com/junkunyuan"><img src="resource/github.png" width="80px" style="margin-bottom:-5px" alt=''></a>] 
    <!-- [<a href="resource/cv.pdf"><img src="resource/cv.jpg" width="160px" style="margin-bottom:-5px" alt=''></a>] -->
    </small>
</h2>
<p>
  Junkun Yuan is a fifth/final-year Ph.D. student at <a href="http://www.zju.edu.cn/">Zhejiang University</a>, supervised by Prof. <a href="https://kunkuang.github.io/">Kun Kuang</a>, <a href="https://person.zju.edu.cn/0096005">Lanfen Lin</a>, and <a href="https://person.zju.edu.cn/wufei">Fei Wu</a>.
  <br><br>

  He is currently working as an intern at <a href="https://www.tencent.com/zh-cn/index.html"> Tencent</a>, focusing on multimodal foundation models of <a href="https://hunyuan.tencent.com/"> Hunyuan <img src="resource/hunyuan2.png" width="80px" style="margin-bottom:-5px" alt=''></a>, with <a href="https://scholar.google.com.hk/citations?user=AjxoEpIAAAAJ&hl=zh-CN&oi=ao"> Wei Liu</a>.
  <br><br>

  During July 2022 - August 2023, he was a research intern at <a href="http://vis.baidu.com/#/"> Baidu Computer Vision Group (VIS)</a>, mentored by <a href="https://jingdongwang2017.github.io/"> Jingdong Wang</a>.
  <br><br>

  His current research interests revolve around the development and application of vision and multimodal foundation models.
</p>


<h2>Publications <small>
    [<a href="https://scholar.google.com.hk/citations?hl=zh-CN&user=j3iFVPsAAAAJ"><img src="resource/google_scholar.png" width="140px" style="margin-bottom:-5px" alt=''></a>] 
    [<a href="https://www.semanticscholar.org/author/Junkun-Yuan/38511927"><img src="resource/semantic_scholar.png" width="140px" style="margin-bottom:-5px" alt=''></a>] 
    [<a href="https://dblp.uni-trier.de/pid/238/0171.html"><img src="resource/dblp.png" width="120px" style="margin-bottom:-15px" alt=''></a>]</small>
</h2>
<br>
<p class="author-pub-normal"> (# Equal Contribution; * Corresponding Author.) </p>

<h3 align="center">2023</h3>

<!-- NeurIPS 2023 HAP -->
<div class="pubs">
    <div class="pub-img-box">
        <img class="pub-img" src="papers/NeurIPS/NeurIPS2023-HAP/NeurIPS2023-HAP-framework.png" width="200px" height="210px"/>
    </div>
    <div class="pub-text-box"><div class="pub-text">
        <p class="title-normal">HAP: Structure-Aware Masked Image Modeling for Human-Centric Perception.</p>
        <p class="author-pub-small"><b>Junkun Yuan</b><sup>#</sup>, Xinyu Zhang<sup>#*</sup>, Hao Zhou, Jian Wang, Zhongwei Qiu, Zhiyin Shao, Shaofeng Zhang, Sifan Long, Kun Kuang<sup>*</sup>, Kun Yao, Junyu Han, Errui Ding, Lanfen Lin, Fei Wu, Jingdong Wang<sup>*</sup>.</p>
        <i class="author-pub-normal"> Conference on Neural Information Processing Systems (<b>NeurIPS</b>), 2023</i> <br><br>
        <i class="abstext">
            <b>(visual self-supervised learning)</b> It first introduces masked image modeling as a pre-training framework for human-centric perception. It incorporates human structure priors into pre-training by proposing a pose-guided masking sampling strategy and a human structure-invarint alignment loss.
        </i>
        <br>
        <br>
        [<a href="https://arxiv.org/abs/2310.20695"><span class="papertext">arXiv</span></a>]
        [<a href="papers/NeurIPS/NeurIPS2023-HAP/NeurIPS2023-HAP.pdf"><span class="papertext">PDF</span></a>]
        [<a href="https://github.com/junkunyuan/HAP"><span class="papertext">Code</span></a>]
        [<a href="https://zhangxinyu-xyz.github.io/hap.github.io/"><span class="papertext">Project</span></a>]
        [<a href="papers/NeurIPS/NeurIPS2023-HAP/HAP-poster.pdf"><span class="papertext">Poster</span></a>]
    </div></div>
</div>

<!-- TKDE 2023 CSAC -->
<div class="pubs">
    <div class="pub-img-box">
        <img class="pub-img" src="papers/TKDE/TKDE-2023-CSAC/TKDE-2023-CSAC-framework.jpg" width="200px" height="175px"/>
    </div>
    <div class="pub-text-box"><div class="pub-text">
        <p class="title-small">Collaborative Semantic Aggregation and Calibration for Federated Domain Generalization.</p>
        <p class="author-pub-normal"><b>Junkun Yuan</b><sup>#</sup>, Xu Ma<sup>#</sup>, Defang Chen, Fei Wu, Lanfen Lin, Kun Kuang<sup>*</sup>.</p>
        <i class="author-pub-normal"> IEEE Transactions on Knowledge and Data Engineering (<b>TKDE</b>), 2023</i> <br><br>
        <i class="abstext">
            <b>(out-of-distribution generalization)</b> To protect data privacy, it solves the federated domain generalization task by proposing a collaborative semantic aggregation and calibration method with local semantic acquisition, data-free semantic aggregation, and cross-layer semantic calibration.
        </i>
        <br>
        <br>
        [<a href="https://arxiv.org/abs/2110.06736"><span class="papertext">arXiv</span></a>]
        [<a href="papers/TKDE/TKDE-2023-CSAC/TKDE-2023-CSAC-paper.pdf"><span class="papertext">PDF</span></a>]
        [<a href="https://github.com/junkunyuan/CSAC"><span class="papertext">Code</span></a>]
    </div></div>
</div>

<!-- TKDD 2023 DRIVE -->
<div class="pubs">
    <span class="pub-img-box">
        <img class="pub-img" src="papers/TKDD/TKDE-2023-iv-dg/TKDD2023-IV-DG-framework.jpg" width="200px" height="200px" alt="" />
    </span>
    <div class="pub-text-box"><div class="pub-text">
        <p class="title-small">Instrumental Variable-Driven Domain Generalization with Unobserved Confounders.</p>
        <p class="author-pub-small"><b>Junkun Yuan</b>, Xu Ma, Ruoxuan Xiong, Mingming Gong, Xiangyu Liu, Fei Wu, Lanfen Lin, Kun Kuang<sup>*</sup>.</p>
        <i class="author-pub-normal"> ACM Transactions on Knowledge Discovery from Data (<b>TKDD</b>), 2023</i> <br><br>
        <i class="abstext">
            <b>(out-of-distribution generalization, causal learning)</b> It provides a causal view on domain generalization that separates domain-invariant and domain-specific parts of data. It then proposes to learn the domain-invariant relationship between the input features and the labels through instrumental variable method. It allows one to remove unobserved confounders and obtain a generalizable model.
        </i>
        <br><br>
        [<a href="https://arxiv.org/abs/2110.01438"><span class="papertext">arXiv</span></a>] 
        [<a href="papers/TKDD/TKDE-2023-iv-dg/TKDE2023-IV-DG-slides.pdf"><span class="papertext">Slides</span></a>] 
    </div></div>
</div>

<!-- TMLR 2023 CAEv2 -->
<div class="pubs">
    <div class="pub-img-box">
        <img class="pub-img" src="papers/TMLR/TMLR2023/TMLR2023-CAEv2/TMLR2023-CAEv2-framework3.png" width="200px" height="200px"/>
    </div>
    <div class="pub-text-box"><div class="pub-text">
        <p class="title-normal">CAE v2: Context Autoencoder with CLIP Target.</p>
        <p class="author-pub-normal">Xinyu Zhang, Jiahui Chen, <b>Junkun Yuan</b>, Qiang Chen, Jian Wang, Xiaodi Wang, Shumin Han, Xiaokang Chen, Jimin Pi, Kun Yao, Junyu Han, Errui Ding, Jingdong Wang<sup>*</sup>.</p>
        <i class="author-pub-normal"> Transactions on Machine Learning Research (<b>TMLR</b>), 2023</i> <br><br>
        <i class="abstext">
            <b>(visual self-supervised learning)</b> It proposes CAE v2, a new CLIP-guided masked image modeling method, which (i) uses both masked and visible tokens as the distillation target, and (ii) employs a masking ratio proportional to the model size, achieving superior performance on downstream tasks.
        </i>
        <br><br>
        [<a href="https://arxiv.org/abs/2211.09799"><span class="papertext">arXiv</span></a>]
    </div></div>
</div>

<!-- ICCV 2023 MAP -->
<div class="pubs">
    <div class="pub-img-box">
        <img class="pub-img" src="papers/ICCV/ICCV2023-MAP/ICCV2023-MAP-framework.jpg" width="200px" height="200px"/>
    </div>
    <div class="pub-text-box"><div class="pub-text">
        <p class="title-small">MAP: Towards Balanced Generalization of IID and OOD through Model-Agnostic Adapters.</p>
        <p class="author-pub-normal">Min Zhang<sup>#</sup>, <b>Junkun Yuan</b><sup>#</sup>, Yue He, Wenbin Li, Zhengyu Chen, Kun Kuang<sup>*</sup>.</p>
        <i class="author-pub-normal">International Conference on Computer Vision (<b>ICCV</b>), 2023</i> <br><br>
        <i class="abstext">
            <b>(out-of-distribution generalization)</b> It presents empirical evidence that existing OOD methods exhibit subpar performance when faced with minor distribution shifts, and proposes a bilevel optimization strategy, leveraging adapters to strike a balance between IID and OOD generalization.
        </i>
        <br><br>
        [<a href="papers/ICCV/ICCV2023-MAP/ICCV2023-MAP.pdf"><span class="papertext">PDF</span></a>]
    </div></div>
</div>

<!-- ICCV 2023 CAM -->
<div class="pubs">
    <div class="pub-img-box">
        <img class="pub-img" src="papers/ICCV/ICCV2023-CAM/ICCV2023-CAM-framework.jpg" width="200px" height="160px"/>
    </div>
    <div class="pub-text-box"><div class="pub-text">
        <p class="title-normal">Universal Domain Adaptation via Compressive Attention Matching.</p>
        <p class="author-pub-normal">Didi Zhu, Yinchuan Li, <b>Junkun Yuan</b>, Zexi Li, Kun Kuang, Chao Wu.</p>
        <i class="author-pub-normal">International Conference on Computer Vision (<b>ICCV</b>), 2023</i> <br><br>
        <i class="abstext">
            <b>(domain adaptation)</b> To better align common features and separate target classes for universal domain adaptation, it makes attempts of leveraging attention maps in ViTs to perform prediction. 
        </i>
        <br><br>
        [<a href="https://arxiv.org/abs/2304.11862"><span class="papertext">arXiv</span></a>]
        [<a href="papers/ICCV/ICCV2023-CAM/ICCV2023-CAM.pdf"><span class="papertext">PDF</span></a>]
    </div></div>
</div>

<!-- ICCV 2023 CTP-TFT -->
<div class="pubs">
    <div class="pub-img-box">
        <img class="pub-img" src="papers/ICCV/ICCV2023-CTP-TFT/arXiv2023-CTP-TFT-framework.png" width="200px" height="220px"/>
    </div>
    <div class="pub-text-box"><div class="pub-text">
        <p class="title-normal">Task-Oriented Multi-Modal Mutual Leaning for Vision-Language Models.</p>
        <p class="author-pub-normal">Sifan Long<sup>#</sup>, Zhen Zhao<sup>#</sup>, <b>Junkun Yuan</b><sup>#</sup>, Zichang Tan, Jiangjiang Liu, Luping Zhou, Shengsheng Wang<sup>*</sup>, Jingdong Wang<sup>*</sup>.</p>
        <i class="author-pub-normal">International Conference on Computer Vision (<b>ICCV</b>), 2023</i> <br><br>
        <i class="abstext">
            <b>(prompt learning)</b> It presents a class-aware text prompt to enrich generated prompts with label-related image information, and makes the image branch attend to class-related representations. The text and image branches mutually promote to enhance the adaptation of vision-language models. 
        </i>
        <br><br>
        [<a href="https://arxiv.org/abs/2303.17169"><span class="papertext">arXiv</span></a>]
        [<a href="papers/ICCV/ICCV2023-CTP-TFT/ICCV2023-CTP-TFT.pdf"><span class="papertext">PDF</span></a>]
    </div></div>
</div>

<!-- KDD 2023 HeterogeneityDG -->
<div class="pubs">
    <span class="pub-img-box">
        <img class="pub-img" src="papers/KDD/KDD-2023-HeterogeneityDG/HeterogeneityDG.png" width="200px" height="160px" alt="" />
    </span>
    <div class="pub-text-box"><div class="pub-text">
        <p class="title-tiny">Quantitatively Measuring and Contrastively Exploring Heterogeneity for Domain Generalization.</p>
        <p class="author-pub-normal">Yunze Tong<sup>#</sup>, <b>Junkun Yuan</b><sup>#</sup>, Min Zhang, Didi Zhu, Keli Zhang, Fei Wu, Kun Kuang<sup>*</sup>.</p>
        <i class="author-pub-normal">ACM SIGKDD Conference on Knowledge Discovery and Data Mining (<b>KDD</b>), 2023</i> <br><br>
        <i class="abstext">
            <b>(out-of-distribution generalization)</b> It tackles the label heterogeneity problem, i.e., the ground-truth labels could be sub-optimal, and proposes a method to quantitatively measure and address it.
        </i>
        <br><br>
        [<a href="https://arxiv.org/abs/2305.15889"><span class="papertext">arXiv</span></a>]
        [<a href="papers/KDD/KDD-2023-HeterogeneityDG/KDD-2023-HeterogeneityDG.pdf"><span class="papertext">PDF</span></a>]
        [<a href="https://github.com/YunzeTong/HTCL"><span class="papertext">Code</span></a>]
    </div></div>
</div>

<!-- TMM 2023 KDDRL -->
<div class="pubs">
    <div class="pub-img-box">
        <img class="pub-img" src="papers/TMM/TMM-2023/TMM-2023-KDDRL-framework.png" width="200px" height="160px"/>
    </div>
    <div class="pub-text-box"><div class="pub-text">
        <p class="title-tinyy">Knowledge Distillation-based Domain-invariant Representation Learning for Domain Generalization.</p>
        <p class="author-pub-small">Ziwei Niu, <b>Junkun Yuan</b>, Xu Ma, Yingying Xu, Jing Liu, Yen-Wei Chen, Ruofeng Tong, Lanfen Lin<sup>*</sup>.</p>
        <i class="author-pub-normal">IEEE Transactions on Multimedia (<b>TMM</b>), 2023</i> <br><br>
        <i class="abstext">
            <b>(out-of-distribution generalization)</b> Inspired by knowledge distillation, it performs two-stage distillation with multiple students, where each student is trained on one dataset, and one leader student. 
        </i>
        <br><br>
        [<a href="papers/TMM/TMM-2023/TMM-2023-KDDRL.pdf"><span class="papertext">PDF</span></a>]
    </div></div>
</div>

<!-- arXiv 2023 NPT -->
<div class="pubs">
    <div class="pub-img-box">
        <img class="pub-img" src="papers/arXiv/arXiv2023-NPT/arXiv2023-NPT.png" width="200px" height="190px"/>
    </div>
    <div class="pub-text-box"><div class="pub-text">
        <p class="title-normal">Understanding Prompt Tuning for V-L Models Through the Lens of Neural Collapse.</p>
        <p class="author-pub-tiny">Didi Zhu, Zexi Li, Min Zhang, <b>Junkun Yuan</b>, Yunfeng Shao, Jiashuo Liu, Kun Kuang, Yinchuan Li, Chao Wu.</p>
        <i class="author-pub-normal">Under review</i> <br><br>
        <i class="abstext">
            <b>(prompt learning)</b> It makes the first attempt to borrow the idea of neural collapes for improving the representations of multimodal models by means of prompt learning. It lets multimodal representations exhibit a equiangular tight frame structure to improve the generalization performance of the model.
        </i>
        <br><br>
        [<a href="https://arxiv.org/abs/2306.15955"><span class="papertext">arXiv</span></a>]
    </div></div>
</div>

<!-- This research introduces a novel approach called neural collapse to enhance the representations of vision-language models by means of prompt learning. By leveraging equiangular tight frame structure, it enables multimodal representations to exhibit improved generalization capabilities. -->

<h3 align="center">2022</h3>

<!-- IJCV 2022 DSBF -->
<div class="pubs">
    <div class="pub-img-box">
        <img class="pub-img" src="papers/IJCV/IJCV2022-DSBF/IJCV2022-DSBF-framework.jpg" width="200px" height="160px"/>
    </div>
    <div class="pub-text-box"><div class="pub-text">
        <p class="title-normal">Domain-Specific Bias Filtering for Single Labeled Domain Generalization.</p>
        <p class="author-pub-normal"><b>Junkun Yuan</b><sup>#</sup>, Xu Ma<sup>#</sup>, Defang Chen, Kun Kuang<sup>*</sup>, Fei Wu, Lanfen Lin.</p>
        <i class="author-pub-normal">International Journal of Computer Vision (<b>IJCV</b>), 2022</i> <br><br>
        <i class="abstext">
            <b>(out-of-distribution generalization)</b> It proposes a task called single labeled domain generalization where only one source dataset being labeled, and a novel method named domain-specific bias filtering.
        </i>
        <br><br>
        [<a href="https://arxiv.org/abs/2110.00726"><span class="papertext">arXiv</span></a>]
        [<a href="papers/IJCV/IJCV2022-DSBF/IJCV2022-DSBF.pdf"><span class="papertext">PDF</span></a>]
        [<a href="https://github.com/junkunyuan/DSBF"><span class="papertext">Code</span></a>]
    </div></div>
</div>

<!-- MM 2022 CEG -->
<div class="pubs">
    <span class="pub-img-box">
        <img class="pub-img" src="papers/MM/MM2022-CEG/MM2022-CEG-framework.jpg" width="200px" height="180px"/>
    </span>
    <div class="pub-text-box"><div class="pub-text">
        <p class="title-small">Label-Efficient Domain Generalization via Collaborative Exploration and Generalization.</p>
        <p class="author-pub-normal"><b>Junkun Yuan</b><sup>#</sup>, Xu Ma<sup>#</sup>, Defang Chen, Kun Kuang<sup>*</sup>, Fei Wu, Lanfen Lin.</p>
        <i class="author-pub-normal">International Conference on Multimedia (<b>MM</b>), 2022</i> <br><br>
        <i class="abstext">
            <b>(out-of-distribution generalization)</b> To enable generalization learning with limited annotation, it proposes a framework called collaborative exploration and generalization, which jointly optimizes active exploration and semi-supervised generalization for the label-efficient domain generalization task.
        </i>
        <br><br>
        [<a href="https://arxiv.org/abs/2208.03644"><span class="papertext">arXiv</span></a>]
        [<a href="papers/MM/MM2022-CEG/MM2022-CEG.pdf"><span class="papertext">PDF</span></a>]
        [<a href="https://github.com/junkunyuan/CEG"><span class="papertext">Code</span></a>]
    </div></div>
</div>

<!-- TKDE 2022 DeR-CFR -->
<div class="pubs">
    <span class="pub-img-box">
        <img class="pub-img" src="papers/TKDE/TKDE2022-DeR-CFR/TKDE2022-DeRCFR-framework.jpg" width="200px" height="170px"/>
    </span>
    <div class="pub-text-box"><div class="pub-text">
        <p class="title-normal">Learning Decomposed Representations for Treatment Effect Estimation.</p>
        <p class="author-pub-tiny">Anpeng Wu<sup>#</sup>, <b>Junkun Yuan</b><sup>#</sup>, Kun Kuang<sup>*</sup>, Bo Li, Pan Zhou, Jianrong Tao, Qiang Zhu, Yueting Zhuang, Fei Wu.</p>
        <i class="author-pub-normal">IEEE Transactions on Knowledge and Data Engineering (<b>TKDE</b>), 2022</i> <br><br>
        <i class="abstext">
            <b>(causal reasoning)</b> It separates confounders by learning representations of confounders and non-confounders, balancing confounders with sample re-weighting, and estimating treatment effect.
        </i>
        <br><br>
        [<a href="https://arxiv.org/abs/2006.07040"><span class="papertext">arXiv</span></a>]
        [<a href="papers/TKDE/TKDE2022-DeR-CFR/TKDE2022-DeRCFR.pdf"><span class="papertext">PDF</span></a>]
        [<a href="https://github.com/anpwu/DeR-CFR"><span class="papertext">Code</span></a>]
    </div></div>
</div>

<!-- TKDD 2022 AutoIV -->
<div class="pubs">
    <span class="pub-img-box">
        <img class="pub-img" src="papers/TKDD/TKDD2022-autoiv/TKDD-2022-autoiv-framework.png" width="200px" height="185px" alt="" />
    </span>
    <div class="pub-text-box"><div class="pub-text">
        <p class="title-small">Auto IV: Counterfactual Prediction via Automatic Instrumental Variable Decomposition.</p>
        <p class="author-pub-normal"><b>Junkun Yuan</b><sup>#</sup>, Anpeng Wu<sup>#</sup>, Kun Kuang<sup>*</sup>, Bo Li, Runze Wu, Fei Wu, Lanfen Lin.</p>
        <i class="author-pub-normal"> ACM Transactions on Knowledge Discovery from Data (<b>TKDD</b>), 2022</i> 
        <br><br>
        <i class="abstext">
            <b>(causal reasoning)</b> 
            Instrumental Variable (IV) is a powerful tool for causal inference, but it is hard to pre-define valid IVs. 
            It proposes an automatic instrumental variable decomposition algorithm to generate effective IV representations from observed variables for IV-based counterfactual prediction.
        </i>
        <br><br>
        [<a href="https://arxiv.org/abs/2107.05884"><span class="papertext">arXiv</span></a>]
        [<a href="papers/TKDD/TKDD2022-autoiv/TKDD-2022-autoiv.pdf"><span class="papertext">PDF</span></a>]
        [<a href="https://github.com/junkunyuan/AutoIV"><span class="papertext">Code</span></a>]
    </div></div>
</div>

<!-- Neurcomputing 2022 ACDA -->
<div class="pubs">
    <span class="pub-img-box">
        <img class="pub-img" src="papers/Neurocomputing/Neurocomputing2022-ACDA/Neurocomputing2022-ACDA-framework.jpg" width="200px" height="185px" alt="" />
    </span>
    <div class="pub-text-box"><div class="pub-text">
        <p class="title-small">Attention-based Cross-Layer Domain Alignment for Unsupervised Domain Adaptation.</p>
        <p class="author-pub-normal">Xu Ma, <b>Junkun Yuan</b>, Yen-wei Chen, Ruofeng Tong, Lanfen Lin<sup>*</sup>.</p>
        <i class="author-pub-normal"><b>Neurocomputing</b>, 2022</i> 
        <br><br>
        <i class="abstext">
            <b>(domain adaptation)</b> 
            It proposes attention-based cross-layer domain alignment, which captures the semantic relationship between the source domain and the target domain across model layers and calibrates each level of semantic information automatically through a dynamic attention mechanism.
        </i>
        <br><br>
        [<a href="https://arxiv.org/abs/2202.13310"><span class="papertext">arXiv</span></a>]
        [<a href="papers/Neurocomputing/Neurocomputing2022-ACDA/Neurocomputing2022-ACDA.pdf"><span class="papertext">PDF</span></a>]
        [<a href="https://github.com/MaXuSun/ACDA"><span class="papertext">Code</span></a>]
    </div></div>
</div>

<!-- ArXiv 2022 CCM -->
<div class="pubs">
    <div class="pub-img-box">
        <img class="pub-img" src="papers/arXiv/arXiv2022-CCM/arXiv2022-CCM-framework.jpg" width="200px" height="160px"/>
    </div>
    <div class="pub-text-box"><div class="pub-text">
        <p class="title-normal">Domain Generalization via Contrastive Causal Learning.</p>
        <p class="author-pub-normal">Qiaowei Miao, <b>Junkun Yuan</b>, Kun Kuang.</p>
        <i class="author-pub-normal">Under review</i> <br><br>
        <i class="abstext">
            <b>(out-of-distribution generalization, causal learning)</b> It proposes a generalizable causal model by controlling the unstable domain factor and quantifying the causal effects with front-door criterion.
        </i>
        <br><br>
        [<a href="https://arxiv.org/abs/2210.02655"><span class="papertext">arXiv</span></a>]
        [<a href="https://github.com/MiaoQiaowei/CCM"><span class="papertext">Code</span></a>]
    </div></div>
</div>

<h3 align="center">2021</h3>

<!-- TKDE 2021 SGN -->
<div class="pubs">
    <div class="pub-img-box">
        <img class="pub-img" src="papers/TKDE/TKDE2021-SGN/TKDE2021-SGN-framwork.jpg" width="200px" height="175px"/>
    </div>
    <div class="pub-text-box"><div class="pub-text">
        <p class="title-normal">Subgraph Networks with Application to Structural Feature Space Expansion.</p>
        <p class="author-pub-small">Qi Xuan<sup>*</sup>, Jinhuan Wang, Minghao Zhao, <b>Junkun Yuan</b>, Chenbo Fu, Zhongyuan Ruan<sup>*</sup>, Guanrong Chen.</p>
        <i class="author-pub-normal">IEEE Transactions on Knowledge and Data Engineering (<b>TKDE</b>), 2021 </i> 
        <br><br>
        <i class="abstext">
            <b>(network embedding)</b> It introduces a new concept of subgraph network and constructs 1st/2nd-order SGNs. SGNs provide complementary structural features for the features that extracted by advanced network-embedding methods, significantly improving performance of network classification.
        </i>
        <br><br>
        [<a href="https://arxiv.org/abs/1903.09022"><span class="papertext">arXiv</span></a>] 
        [<a href="papers/TKDE/TKDE2021-SGN/TKDE-2021-SGN.pdf"><span class="papertext">PDF</span></a>]
        [<a href="https://github.com/GalateaWang/SGNs-master"><span class="papertext">Code</span></a>]
    </div></div>
</div>

<h3 align="center">2020
</h3>

<!-- ECAI 2020 GAPGAN -->
<div class="pubs">
    <div class="pub-img-box">
        <img class="pub-img" src="papers/ECAI/ECAI2020-GAPGAN/ECAI2020-GAPGAN-framework.jpg" width="200px" height="175px">
    </div>
    <div class="pub-text-box"><div class="pub-text">
        <p class="title-tinyy">Black-box Adversarial Attacks Against Deep Learning Based Malware Binaries Detection with GAN.</p>
        <p class="author-pub-normal"><b>Junkun Yuan</b>, Shaofang Zhou, Lanfen Lin<sup>*</sup>, Feng Wang, Jia Cui.</p>
        <i class="author-pub-normal">European Conference on Artificial Intelligence (<b>ECAI</b>), 2020 </i> 
        <br><br>
        <i class="abstext">
            <b>(adversarial attack)</b> To attack malware detector, it proposes a black-box attack framework called GAPGAN to generate adversarial payloads with GAN. It generates payloads with malware binaries, and appends them to the binaries to craft adversarial samples while preserving the original functionality.
        </i>
        <br><br>
        [<a href="papers/ECAI/ECAI2020-GAPGAN/ECAI2020-GAPGAN.pdf"><span class="papertext">PDF</span></a>]
        [<a href="papers/ECAI/ECAI2020-GAPGAN/ECAI2020-GAPGAN-slides.pdf"><span class="papertext">Slides</span></a>]
    </div></div>
</div>

<h3 align="center">2019</h3>

<!-- ISI 2019 CNN -->
<div class="pubs">
    <div class="pub-img-box">
        <img class="pub-img" src="papers/ISI/ISI2019/ISI2019-DGA-framework.jpg" width="200px" height="160px">
    </div>
    <div class="pub-text-box"><div class="pub-text">
        <p class="title-normal">CNN-based DGA Detection with High Coverage.</p>
        <p class="author-pub-normal">Shaofang Zhou, Lanfen Lin<sup>*</sup>, <b>Junkun Yuan</b>, Feng Wang, Zhaoting Ling, Jia Cui.</p>
        <i class="author-pub-normal">International Conference on Intelligence and Security Informatics (<b>ISI</b>), 2019</i> <br><br>
        <i class="abstext">
            <b>(adversarial attack)</b> Attackers use algorithms to create domains for attacks. It presents a temporal convolutional network-based generated domain detection method with high accuracy and coverage.
        </i>
        <br><br>
        [<a href="papers/ISI/ISI2019/ISI2019-DGA.pdf"><span class="papertext">PDF</span></a>] 
    </div></div>
</div>


<h2>Professional Services</h2>
<span>Conference reviewer: AAAI 2023, MM 2023, ICCV 2023.</span>
<br><br>
<span>Journal reviewer: TNNLS, NN, TCSVT.</span>
<!-- <br><br><br> -->
<!-- <span>Help to review for conferences: ICML 2021; KDD 2021; ICCV 2021, 2023; CIKM 2021; NeurIPS 2021; ICDM 2022; AAAI 2021, 2022; ICLR 2023; CVPR 2021, 2022.</span> -->
<br><br>

<div id="footer">
    <div id="footer-text"></div>
</div>

</div>

Last updated on November 1, 2023.
</body>
