<!DOCTYPE html>
<html>

<head>
    <link rel="stylesheet" href="jemdoc_reading_papers.css" type="text/css">
    <link rel="shortcut icon" href="../resource/citations.jpg">
    <title>JunkunYuan's Reading Papers</title>
    <meta name="description" content="Junkun Yuan&#39;s Reading Papers">
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    
    <div id="layout-content" style="margin-top:25px"></div>

    <style>
        table {
            width: 100%;
            border-collapse: collapse;
            margin: 20px 0;
        }
        th, td {
            border: 1px solid #adadad;
            text-align: left;
            padding: 8px;
        }
        th {
            background-color: #e2e2e2;
        }
        
        .image-container {
            display: flex;
            justify-content: center;
            margin: 20px;
        }
        figure {
            margin: 0 10px;
            text-align: center;
        }
        img {
            max-width: 100%;
            height: auto;
        }
        figcaption {
            font-size: 14px;
            color: #CC6600;
        }
        
        .papertext {
            color: #D93053;
            font-size: 15px;
        }

        .summarytext {
            color: #000000;
            font-size: 15px;
        }

        .datetext {
            color: #888888;
            font-size: 15px;
        }

        .modeltext {
            color: #224bad;
            font-size: 15px;
        }

        .notetext {
            color: #129b1d;
            font-size: 15px;
            font-weight: bold;
        }
    </style>
</head>

<body>

<h1 id="top">Visual Generative Models</h1>

<b><font size=4>Curated by Junkun Yuan (yuanjk0921@outlook.com)</font></b>

<br><br>
<b>Contents:</b>
<ul>
    <li><a href="#foundation-model-and-algorithms">Foundation Models and Algorithms</a></li>
    <li><a href="#finetuning">Fine-Tuning</a></li>
    <li><a href="#reinforcement-learning">Reinforcement Learning</a></li>
    <li><a href="#inference-time-improvement">Inference-Time Improvement</a></li>
    <li><a href="#evaluation">Evaluation</a></li>
    <li><a href="#papers">Papers & Reading Notes (total: 23)</a></li>
</ul>

<br>
<a href="reading_papers.html">[back to main contents]</a>

<br><br>
Last updated on Feb. 16, 2025.



<!-- Section: Foundation Models and Algorithms -->
<h2 id="survey-and-analysis">Surveys and Insights</h2>

Surveys and interesting insights on visual generative models.

<table>
    <colgroup>
        <col style="width: 16%;">
        <col style="width: 36%;">
        <col style="width: 48%;">
    </colgroup>
    <thead>
        <tr>
            <th>Date & Model</th>
            <th>Paper & Publication & Project</th>
            <th>Summary: perspective</th>
        </tr>
    </thead>
    <tbody>
        <!-- 2022-08-25-Unified-Perspective -->
        <tr>
            <td><span class="datetext">2022-08-25</span><br><a href="#2022-08-25-Unified-Perspective" class="modeltext"><b>Unified Perspective</b></a></td>
            <td>
                <a href="https://arxiv.org/pdf/2412.03603"><span class="papertext">
                    Understanding Diffusion Models: A Unified Perspective <i>(arXiv 2022)</i>
                </span></a> 
            </td>
            <td><span class="summarytext">
                Introduction to <u>VAE</u>, <u>DDPM</u>, <u>score-based generative model</u>, <u>guidance</u> from a <b>unified generative perspective<b>.
            </span></td>
        </tr>
    </tbody>
</table>

<a href="#top">[back to top]</a>




<!-- Section: Foundation Models and Algorithms -->
<h2 id="foundation-model-and-algorithms">Foundation Models and Algorithms</h2>

Foundation models and algorithms on visual generative models.

<table>
    <colgroup>
        <col style="width: 16%;">
        <col style="width: 36%;">
        <col style="width: 48%;">
    </colgroup>
    <thead>
        <tr>
            <th>Date & Model</th>
            <th>Paper & Publication & Project</th>
            <th>Summary: task & model strucuture & training receipe</th>
        </tr>
    </thead>
    <tbody>
        <!-- 2024-12-03-hunyuanvideo -->
        <tr>
            <td><span class="datetext">2024-12-03</span><br><a href="#2024-12-03-hunyuanvideo" class="modeltext"><b>HunyuanVideo</b></a></td>
            <td>
                <a href="https://arxiv.org/pdf/2412.03603"><span class="papertext">
                    HunyuanVideo: A Systematic Framework For Large Video Generative Models <i>(arXiv 2024)</i>
                </span></a> 
                <a href="https://github.com/Tencent/HunyuanVideo/"><img src="https://img.shields.io/github/stars/Tencent/HunyuanVideo.svg?style=social&label=Star" alt="Star" style="vertical-align: middle;" /></a>
            </td>
            <td><span class="summarytext">
                An <u>open-sourced</u> <b>video generation</b> model (13B) with the structure of <b>diffusion transformer</b>.
            </span></td>
        </tr>
        <!-- 2022-12-19-DiT -->
        <tr>
            <td><span class="datetext">2022-12-19</span><br><a href="#2022-12-19-DiT" class="modeltext"><b>DiT</b></a></td>
            <td>
                <a href="https://arxiv.org/pdf/2212.09748"><span class="papertext">
                    Scalable Diffusion Models with Transformers <i>(ICCV 2023)</i>
                </span></a> 
                <a href="https://github.com/facebookresearch/DiT/"><img src="https://img.shields.io/github/stars/facebookresearch/DiT.svg?style=social&label=Star" alt="Star" style="vertical-align: middle;" /></a>
            </td>
            <td><span class="summarytext">
                Replace U-Net by <b>transformer</b> for scalable <b>image generation</b>, timestep and prompt are injected by adaLN-Zero.
            </span></td>
        </tr>
        <!-- 2022-05-29-cogvideo -->
        <tr>
            <td><span class="datetext">2022-05-29</span><br><a href="#2022-05-29-cogvideo" class="modeltext">CogVideo</a></td>
            <td>
                <a href="https://arxiv.org/pdf/2205.15868"><span class="papertext">
                    CogVideo: Large-scale Pretraining for Text-to-Video Generation via Transformers <i>(ICLR 2023)</i>
                </span></a> 
                <a href="https://github.com/THUDM/CogVideo/"><img src="https://img.shields.io/github/stars/THUDM/CogVideo.svg?style=social&label=Star" alt="Star" style="vertical-align: middle;" /></a>
            </td>
            <td><span class="summarytext">
                An <u>open-sourced</u> <b>transformer</b>-based <b>video generation</b> model (9B) that <b>auto-regressively</b> generates frame sequences and then performs frame interpolatation.
            </span></td>
        </tr>
        <!-- 2021-12-20-ldm -->
        <tr>
            <td><span class="datetext">2021-12-20</span><br><a href="#2021-12-20-ldm" class="modeltext"><b>LDM</b></a></td>
            <td>
                <a href="https://arxiv.org/pdf/2112.10752"><span class="papertext">
                    High-Resolution Image Synthesis with Latent Diffusion Models <i>(CVPR 2022)</i>
                </span></a> 
                <a href="https://github.com/CompVis/latent-diffusion/"><img src="https://img.shields.io/github/stars/CompVis/latent-diffusion.svg?style=social&label=Star" alt="Star" style="vertical-align: middle;" /></a>
            </td>
            <td><span class="summarytext">
                Efficient high-quality <b>image generation</b> by applying <b>U-Net diffusion models</b> in the <b>VAE latent space</b>.
            </span></td>
        </tr>
        <!-- 2020-12-16-ddpm -->
        <tr>
            <td><span class="datetext">2020-12-16</span>
                <br><a href="#2020-12-16-ddpm" class="modeltext"><b>DDPM</b></a>
                <br><a href="https://github.com/junkunyuan/junkunyuan.github.io/blob/master/jupyters_reading_papers/visual_generative_models-ddpm.ipynb" class="notetext">(learning notes)</a>
            </td>
            <td>
                <a href="https://arxiv.org/pdf/2006.11239"><span class="papertext">
                    Denoising Diffusion Probabilistic Models <i>(NeurIPS 2020)</i>
                </span></a> 
                <a href="https://github.com/hojonathanho/diffusion/"><img src="https://img.shields.io/github/stars/hojonathanho/diffusion.svg?style=social&label=Star" alt="Star" style="vertical-align: middle;" /></a>
            </td>
            <td><span class="summarytext">
                Denoising <b>diffusion</b> probabilistic models that iteratively denoises data from random noise for <b>image generation</b>.
            </span></td>
        </tr>
    </tbody>
</table>

<a href="#top">[back to top]</a>




<!-- Section: Fine-Tuning -->
<h2 id="finetuning">Fine-Tuning</h2>

Post-training by fine-tuning models or some modules like LoRA or ControlNet.

<table>
    <colgroup>
        <col style="width: 16%;">
        <col style="width: 36%;">
        <col style="width: 48%;">
    </colgroup>
    <thead>
        <tr>
            <th>Date & Model</th>
            <th>Paper & Publication & Project</th>
            <th>Summary: task & training strategies</th>
        </tr>
    </thead>
    <tbody>
        <!-- 2024-03-08-DPG-Bench -->
        <tr>
            <td><span class="datetext">2024-03-08</span><br><a href="#2024-03-08-DPG-Bench" class="modeltext">ELLA</a></td>
            <td>
                <a href="https://arxiv.org/pdf/2403.05135"><span class="papertext">ELLA: Equip Diffusion Models with LLM for Enhanced Semantic Alignment <i>(arXiv 2024)</i></span></a> 
                <a href="https://github.com/TencentQQGYLab/ELLA/"><img src="https://img.shields.io/github/stars/TencentQQGYLab/ELLA.svg?style=social&label=Star" alt="Star" style="vertical-align: middle;" /></a>
            </td>
            <td><span class="summarytext">
                ELLA: Replace CLIP with <b>LLM</b> to understand dense prompts; DPG-Bench: evaluate <b>image generation</b> on <b>dense prompts</b>.
            </span></td>
        </tr>
    </tbody>
</table>

<a href="#top">[back to top]</a>



<!-- Section: Reinforcement Learning -->
<h2 id="reinforcement-learning">Reinforcement Learning</h2>

Post-training by employing reinforcement learning.

<table>
    <colgroup>
        <col style="width: 16%;">
        <col style="width: 36%;">
        <col style="width: 48%;">
    </colgroup>
    <thead>
        <tr>
            <th>Date & Model</th>
            <th>Paper & Publication & Project</th>
            <th>Summary: task & training strategies</th>
        </tr>
    </thead>
    <tbody>
        <!-- 2025-01-23-cot -->
        <tr>
            <td><span class="datetext">2025-01-23</span><br><a href="#2025-01-23-cot" class="modeltext">PARM</a></td>
            <td>
                <a href="https://arxiv.org/pdf/2501.13926"><span class="papertext">Can We Generate Images with CoT? Let's Verify and Reinforce Image Generation Step by Step <i>(arXiv 2025)</i></span></a> 
                <a href="https://github.com/ZiyuGuo99/Image-Generation-CoT/"><img src="https://img.shields.io/github/stars/ZiyuGuo99/Image-Generation-CoT.svg?style=social&label=Star" alt="Star" style="vertical-align: middle;" /></a>
            </td>
            <td><span class="summarytext">
                Apply <b>Chain-of-Thought</b> into <b>image generation</b> and combine it with reinforcement learning to further improve performance.
            </span></td>
        </tr>
        <!-- 2025-01-23-flowrar -->
        <tr>
            <td><span class="datetext">2025-01-23</span><br><a href="#2025-01-23-flowrar" class="modeltext">Flow-RWR<br>Flow-DPO</a></td>
            <td>
                <a href="https://arxiv.org/pdf/2501.13918"><span class="papertext">Improving Video Generation with Human Feedback <i>(arXiv 2025)</i></span></a>
            </td>
            <td><span class="summarytext">
                A <u>human preference video dataset</u>; Adapt diffusion-based reinforcement learning to <u>flow-based</u> <b>video generation</b> models.
            </span></td>
        </tr>
        <!-- 2023-12-19-InstructVideo -->
        <tr>
            <td><span class="datetext">2023-12-19</span><br><a href="#2023-12-19-InstructVideo" class="modeltext">InstructVideo</a></td>
            <td>
                <a href="https://arxiv.org/pdf/2312.12490"><span class="papertext">
                    InstructVideo: Instructing Video Diffusion Models with Human Feedback <i>(CVPR 2024)</i>
                </span></a>
                <a href="https://github.com/ali-vilab/VGen/"><img src="https://img.shields.io/github/stars/ali-vilab/VGen.svg?style=social&label=Star" alt="Star" style="vertical-align: middle;" /></a>
            </td>
            <td><span class="summarytext">
                Use HPS v2 to provide reward and train <b>video generation</b> models in an editing manner.
            </span></td>
        </tr>
        <!-- 2022-12-19-promptist -->
        <tr>
            <td><span class="datetext">2022-12-19</span><br><a href="#2022-12-19-promptist" class="modeltext">promptist</a></td>
            <td>
                <a href="https://arxiv.org/pdf/2212.09611"><span class="papertext">
                    Optimizing Prompts for Text-to-Image Generation <i>(NeurIPS 2023)</i>
                </span></a>
                <a href="https://github.com/microsoft/LMOps/"><img src="https://img.shields.io/github/stars/microsoft/LMOps.svg?style=social&label=Star" alt="Star" style="vertical-align: middle;" /></a>
            </td>
            <td><span class="summarytext">
                Use language models for <b>image generation</b> <b>prompt refinement</b> by taking relevance and aesthetics as reward.
            </span></td>
        </tr>
    </tbody>
</table>

<a href="#top">[back to top]</a>



<!-- Section: Inference-Time Improvement -->
<h2 id="inference-time-improvement">Inference-Time Improvement</h2>

Improve inference-time visual generation performance, inspired by progress in large language models like <a href="https://openai.com/index/learning-to-reason-with-llms/">OpenAI o1</a> and <a href="https://arxiv.org/pdf/2501.12948">DeepSeek-R1</a>.

<table>
    <colgroup>
        <col style="width: 16%;">
        <col style="width: 36%;">
        <col style="width: 48%;">
    </colgroup>
    <thead>
        <tr>
            <th>Date & Model</th>
            <th>Paper & Publication & Project</th>
            <th>Summary: task & strategies</th>
        </tr>
    </thead>
    <tbody>
        <!-- 2025-01-23-cot -->
        <tr>
            <td><span class="datetext">2025-01-23</span><br><a href="#2025-01-23-cot" class="modeltext">PARM</a></td>
            <td>
                <a href="https://arxiv.org/pdf/2501.13926"><span class="papertext">Can We Generate Images with CoT? Let's Verify and Reinforce Image Generation Step by Step <i>(arXiv 2025)</i></span></a> 
                <a href="https://github.com/ZiyuGuo99/Image-Generation-CoT/"><img src="https://img.shields.io/github/stars/ZiyuGuo99/Image-Generation-CoT.svg?style=social&label=Star" alt="Star" style="vertical-align: middle;" /></a>
            </td>
            <td><span class="summarytext">
                Apply <b>Chain-of-Thought</b> into <b>image generation</b> and combine it with reinforcement learning to further improve performance.
            </span></td>
        </tr>
        <!-- 2025-01-16-scaling-analysis -->
        <tr>
            <td><span class="datetext">2025-01-16</span><br><a href="#2025-01-16-scaling-analysis" class="modeltext">Scaling Analysis</a></td>
            <td>
                <a href="https://arxiv.org/pdf/2501.09732"><span class="papertext">Inference-Time Scaling for Diffusion Models beyond Scaling Denoising Steps <i>(arXiv 2025)</i></span></a>
            </td>
            <td>
                <span class="summarytext">
                    Analysis on inference-time scaling of diffusion models for <b>image generation</b> from the axes of <b>verifiers</b> and <b>algorithms</b>.
                </span>
            </td>
        </tr>
    </tbody>
</table>

<a href="#top">[back to top]</a>



<!-- Section: Evaluation -->
<h2 id="evaluation">Evaluation</h2>

<u>Metrics and benchmarks for evaluating visual generation performance.</u>

<table>
    <colgroup>
        <col style="width: 16%;">
        <col style="width: 36%;">
        <col style="width: 48%;">
    </colgroup>
    <thead>
        <tr>
            <th>Date & Model</th>
            <th>Paper & Publication & Project</th>
            <th>Summary: task & main evaluation focuses</th>
        </tr>
    </thead>
    <tbody>
        <!-- 2024-07-19-t2vcompbench -->
        <tr>
            <td><span class="datetext">2024-07-19</span><br><a href="#2024-07-19-t2vcompbench" class="modeltext"><b>T2V-CompBench</b></a></td>
            <td>
                <a href="https://arxiv.org/pdf/2407.14505"><span class="papertext">T2V-CompBench: A Comprehensive Benchmark for Compositional Text-to-video Generation <i>(arXiv 2024)</i></span></a> 
                <a href="https://github.com/KaiyueSun98/T2V-CompBench/"><img src="https://img.shields.io/github/stars/KaiyueSun98/T2V-CompBench.svg?style=social&label=Star" alt="Star" style="vertical-align: middle;" /></a>
            </td>
            <td><span class="summarytext">
                Use 1400 prompts to evaluate <b>video generation</b> on <b>compositional generation</b>, including consistent attribute binding, dynamic attribute binding, sptial relationships, motion binding, action binding, object interations, generative numeracy.
            </span></td>
        </tr>
        <!-- 2024-03-08-DPG-Bench -->
        <tr>
            <td><span class="datetext">2024-03-08</span><br><a href="#2024-03-08-DPG-Bench" class="modeltext"><b>DPG-Bench</b></a></td>
            <td>
                <a href="https://arxiv.org/pdf/2403.05135"><span class="papertext">ELLA: Equip Diffusion Models with LLM for Enhanced Semantic Alignment <i>(arXiv 2024)</i></span></a> 
                <a href="https://github.com/TencentQQGYLab/ELLA/"><img src="https://img.shields.io/github/stars/TencentQQGYLab/ELLA.svg?style=social&label=Star" alt="Star" style="vertical-align: middle;" /></a>
            </td>
            <td><span class="summarytext">
                ELLA: Replace CLIP with <b>LLM</b> to understand dense prompts; DPG-Bench: evaluate <b>image generation</b> on <b>dense prompts</b>.
            </span></td>
        </tr>
        <!-- 2023-11-29-vbench -->
        <tr>
            <td><span class="datetext">2023-11-29</span><br><a href="#2023-11-29-vbench" class="modeltext"><b>VBench</b></a></td>
            <td>
                <a href="https://arxiv.org/pdf/2311.17982"><span class="papertext">VBench: Comprehensive Benchmark Suite for Video Generative Models <i>(CVPR 2024)</i></span></a> 
                <a href="https://github.com/Vchitect/VBench/"><img src="https://img.shields.io/github/stars/Vchitect/VBench.svg?style=social&label=Star" alt="Star" style="vertical-align: middle;" /></a>
            </td>
            <td><span class="summarytext">
                Evaluate <b>video generation</b> from 16 dimensions within the perspectives of video quality and video-prompt consistency.
            </span></td>
        </tr>
        <!-- 2023-10-17-GenEval -->
        <tr>
            <td><span class="datetext">2023-10-17</span><br><a href="#2023-10-17-GenEval" class="modeltext"><b>GenEval</b></a></td>
            <td>
                <a href="https://arxiv.org/pdf/2310.11513"><span class="papertext">GenEval: An Object-Focused Framework for Evaluating Text-to-Image Alignment <i>(NeurIPS 2023)</i></span></a> 
                <a href="https://github.com/djghosh13/geneval/"><img src="https://img.shields.io/github/stars/djghosh13/geneval.svg?style=social&label=Star" alt="Star" style="vertical-align: middle;" /></a>
            </td>
            <td><span class="summarytext">
                An <b>object-focused</b> framework for <b>image generation</b> evaluation by providing scores of single object, two objects, counting, colors, position, attribute binding, and overall.
            </span></td>
        </tr>
        <!-- 2023-07-12-t2icompbench -->
        <tr>
            <td><span class="datetext">2023-07-12</span><br><a href="#2023-07-12-t2icompbench" class="modeltext"><b>T2I-CompBench</b></a></td>
            <td>
                <a href="https://arxiv.org/pdf/2307.06350"><span class="papertext">T2I-CompBench: A Comprehensive Benchmark for Open-world Compositional Text-to-image Generation <i>(NeurIPS 2023)</i></span></a> 
                <a href="https://github.com/Karine-Huang/T2I-CompBench/"><img src="https://img.shields.io/github/stars/Karine-Huang/T2I-CompBench.svg?style=social&label=Star" alt="Star" style="vertical-align: middle;" /></a>
            </td>
            <td><span class="summarytext">
                Use 6000 prompts to evaluate <b>image generation</b> on <b>compositional generation</b>, including attribute binding, object relationship, and complex compositions.
            </span></td>
        </tr>
        <!-- 2023-06-15-hpsv2 -->
        <tr>
            <td><span class="datetext">2023-06-15</span><br><a href="#2023-06-15-hpsv2" class="modeltext"><b>HPS v2</b></a></td>
            <td>
                <a href="https://arxiv.org/pdf/2306.09341"><span class="papertext">Human Preference Score v2: A Solid Benchmark for Evaluating Human Preferences of Text-to-Image Synthesis <i>(arXiv 2023)</i></span></a> 
                <a href="https://github.com/tgxs002/HPSv2/"><img src="https://img.shields.io/github/stars/tgxs002/HPSv2.svg?style=social&label=Star" alt="Star" style="vertical-align: middle;" /></a>
            </td>
            <td><span class="summarytext">
                HPD v2: 798K binary human preference choices on 433K pairs of generated images; HPS v2: use HPD v2 to fine-tune CLIP for <b>image generation</b> evaluation.
            </span></td>
        </tr>
        <!-- 2023-04-12-imagereward -->
        <tr>
            <td><span class="datetext">2023-04-12</span><br><a href="#2023-04-12-imagereward" class="modeltext"><b>ImageReward</b></a></td>
            <td>
                <a href="https://arxiv.org/pdf/2304.05977"><span class="papertext">ImageReward: Learning and Evaluating Human Preferences for Text-to-Image Generation <i>(NeurIPS 2023)</i></span></a> 
                <a href="https://github.com/THUDM/ImageReward/"><img src="https://img.shields.io/github/stars/THUDM/ImageReward.svg?style=social&label=Star" alt="Star" style="vertical-align: middle;" /></a>
            </td>
            <td><span class="summarytext">
                Train BLIP on 137K human preference image pairs for <b>image generation</b> and use it to tune diffusion models by Reward Feedback Learning (ReFL).
            </span></td>
        </tr>
        <!-- 2023-03-25-hps -->
        <tr>
            <td><span class="datetext">2023-03-25</span><br><a href="#2023-03-25-hps" class="modeltext"><b>HPS</b></a></td>
            <td>
                <a href="https://arxiv.org/pdf/2303.14420"><span class="papertext">Human Preference Score: Better Aligning Text-to-Image Models with Human Preference <i>(ICCV 2023)</i></span></a> 
                <a href="https://github.com/tgxs002/align_sd/"><img src="https://img.shields.io/github/stars/tgxs002/align_sd.svg?style=social&label=Star" alt="Star" style="vertical-align: middle;" /></a>
            </td>
            <td><span class="summarytext">
                Fine-tune CLIP using annotated 98K SD generated images from 25K prompts for <b>image generation</b> evaluation.
            </span></td>
        </tr>
        <!-- 2021-04-18-clipscore -->
        <tr>
            <td><span class="datetext">2021-04-18</span><br><a href="#2021-04-18-clipscore" class="modeltext"><b>CLIPScore</b></a></td>
            <td>
                <a href="https://arxiv.org/pdf/2104.08718"><span class="papertext">CLIPScore: A Reference-free Evaluation Metric for Image Captioning <i>(EMNLP 2021)</i></span></a>
            </td>
            <td><span class="summarytext">
                A reference-free metric mainly focusing on <b>semantic alignment</b> for <b>image generation</b> evaluation.
            </span></td>
        </tr>
        <!-- 2019-05-04-fvd -->
        <tr>
            <td><span class="datetext">2019-05-04</span><br><a href="#2019-05-04-fvd" class="modeltext"><b>FVD</b></a></td>
            <td>
                <a href="https://openreview.net/pdf?id=rylgEULtdN"><span class="papertext">FVD: A new Metric for Video Generation <i>(ICLR workshop 2019)</i></span></a>
            </td>
            <td><span class="summarytext">
                Extend FID for <b>video generation</b> evaluation by replacing 2D InceptionNet with pre-trained Inflated 3D convnet.
            </span></td>
        </tr>
        <!-- 2017-06-26-fid -->
        <tr>
            <td><span class="datetext">2017-06-26</span><br><a href="#2017-06-26-fid" class="modeltext"><b>FID</b></a></td>
            <td>
                <a href="https://arxiv.org/pdf/1706.08500"><span class="papertext">GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium <i>(NeurIPS 2017)</i></span></a>
            </td>
            <td><span class="summarytext">
                Calculate <b>Fréchet distance</b> between Gaussian distributions of InceptionNet feature maps of real-world data and synthetic data for <b>image generation</b> evaluation.
            </span></td>
        </tr>
        <!-- 2016-06-10-inception-score -->
        <tr>
            <td><span class="datetext">2016-06-10</span><br><a href="#2016-06-10-inception-score" class="modeltext"><b>Inception Score</b></a></td>
            <td>
                <a href="https://arxiv.org/pdf/1606.03498"><span class="papertext">Improved Techniques for Training GANs <i>(NeurIPS 2016)</i></span></a> 
                <a href="https://github.com/openai/improved-gan/"><img src="https://img.shields.io/github/stars/openai/improved-gan.svg?style=social&label=Star" alt="Star" style="vertical-align: middle;" /></a>
            </td>
            <td><span class="summarytext">
                Calculate <b>KL divergence between p(y|x) and p(y)</b> that aims to minimize the entropy across samples and maximize the entropy across classes for <b>image generation</b> evaluation.
            </span></td>
        </tr>
    </tbody>
</table>

<a href="#top">[back to top]</a>



<!-- Section: Papers -->
<h2 id="papers">Papers & Reading Notes</h2>

<!-- Beginning of a paper -->
<!-- <h4 id="template">
    [2020-12-16] Template
</h4>
<p>
    <b>Authors:</b>
</p>
<p>
    <b>Organizations:</b>
</p>
<p>
    <b>Summary:</b> 
</p>
<ul>
    <li>
</ul>
<table>
    <tr>
        <td>
            <figure>
                <img src='temp1.png' width=300>
                <figcaption>caption1.</figcaption>
            </figure>
        </td>
        <td>
            <figure>
                <img src='temp2.png' width=600>
        <figcaption>caption2.</figcaption>
            </figure>
        </td>
    </tr>
</table>
<p><a href="#top">[back to top]</a></p> -->
<!-- End of a paper -->

<!-- Beginning of a paper -->
<h4 id="2025-01-23-flowrar">
    [2025-01-23] Improving Video Generation with Human Feedback <i>(arXiv 2025)</i>
</h4>
<p>
    <b>Authors:</b> Jie Liu, Gongye Liu, Jiajun Liang, Ziyang Yuan, Xiaokun Liu, Mingwu Zheng, Xiele Wu, Qiulin Wang, Wenyu Qin, Menghan Xia, Xintao Wang, Xiaohong Liu, Fei Yang, Pengfei Wan, Di Zhang, Kun Gai, Yujiu Yang, Wanli Ouyang
</p>
<p>
    <b>Organizations:</b> CUHK, Tsinghua University, Kuaishou Technology, Shanghai Jiao Tong University, Shanghai AI Lab
</p>
<p>
    <b>Summary:</b> A <u>human preference video dataset</u>; Adapt diffusion-based reinforcement learning to <u>flow-based</u> <b>video generation</b> models.
</p>
<p><a href="#top">[back to top]</a></p>
<!-- End of a paper -->
 

<!-- Beginning of a paper -->
<h4 id="2025-01-23-cot">
    [2025-01-23] Can We Generate Images with CoT? Let's Verify and Reinforce Image Generation Step by Step <i>(arXiv 2025)</i>
</h4>
<p>
    <b>Authors:</b> Ziyu Guo, Renrui Zhang, Chengzhuo Tong, Zhizheng Zhao, Peng Gao, Hongsheng Li, Pheng-Ann Heng
</p>
<p>
    <b>Organizations:</b> CUHK, Peking University, Shanghai AI Lab
</p>
<p>
    <b>Summary:</b> Apply <b>Chain-of-Thought</b> into <b>image generation</b> and combine it with reinforcement learning to further improve performance.
</p>
<ul>
    <li>Test-time verification and DPO are effective, where Outcome Reward Model (ORM) and DPO performs better than Process Reward Model (PRM).</li>
    <li>Verification and DPO are complementary.</li>
    <li>Propose Potential Assessment Reward Model (PARM) to combine the advantages of ORM and PRM.</li>
    <li>Self-correction is useful.</li>
</ul>
<table>
    <tr>
        <td>
            <figure>
                <img src='figs_reading_papers/2025-01-23-cot-fig1.png' width=650>
                <figcaption>ORM is coarse, PRM does not know when to make decision, PARM combines them.</figcaption>
            </figure>
        </td>
        <td>
            <figure>
                <img src='figs_reading_papers/2025-01-23-cot-fig2.png' width=250>
                <figcaption>Self-correction makes use of bad cases.</figcaption>
            </figure>
        </td>
    </tr>
</table>
<p><a href="#top">[back to top]</a></p>
<!-- End of a paper -->


<!-- Beginning of a paper -->
<h4 id="2025-01-16-scaling-analysis">
    [2025-01-16] Inference-Time Scaling for Diffusion Models beyond Scaling Denoising Steps <i>(arXiv 2025)</i>
</h4>
<p>
    <b>Authors:</b> Ma, Shangyuan Tong, Haolin Jia, Hexiang Hu, Yu-Chuan Su, Mingda Zhang, Xuan Yang, Yandong Li, Tommi Jaakkola, Xuhui Jia, Saining Xie
</p>
<p>
    <b>Organizations:</b> NYU, MIT, Google
</p>
<p>
    <b>Summary:</b> Analysis on inference-time scaling of diffusion models for <b>image generation</b> from the axes of <b>verifiers</b> and <b>algorithms</b>.
</p>
<ul>
    <li>Use some <u>verifiers</u> to provide <u>feedback</u>: FID, IS, CLIP, DINO; Aesthetic Score Predictor, CLIPScore, ImageReward, Ensemble.</li>
    <li>Use some <u>algorithms</u> to find <u>better noise</u>: <u>Random Search (the best performance)</u>, Zero-Order Search, Search Over Paths.</li>
    <li>No single verifier-algorithm configuration is universally optimal.</li>
    <li>Inference-time search further improves performance of the model which has already been fine-tuned.</li>
    <li>Smaller search/iter ratio enables efficient convergence but lower final performance.</li>
    <li>With a fixed inference compute budget, <u>performing search on small models can outperform larger models without search</u>.</li>
</ul>
<table>
    <tr>
        <td>
            <figure>
                <img src='figs_reading_papers/2025-01-16-scaling-analysis fig1.png' width=400>
                <figcaption>Scale with search is more effective than with denoising steps.</figcaption>
            </figure>
        </td>
        <td>
            <figure>
                <img src='figs_reading_papers/2025-01-16-scaling-analysis fig2.png' width=500>
        <figcaption>Random Search performs the best because it converges fastest.</figcaption>
            </figure>
        </td>
    </tr>
</table>
<p><a href="#top">[back to top]</a></p>
<!-- End of a paper -->



<!-- Beginning of a paper -->
<h4 id="2024-12-03-hunyuanvideo">
    [2024-12-03] HunyuanVideo: A Systematic Framework For Large Video Generative Models
</h4>
<p>
    <b>Authors:</b> Hunyuan Foundation Model Team
</p>
<p>
    <b>Organizations:</b> Tencent
</p>
<p>
    <b>Summary:</b> An <u>open-sourced</u> <b>video generation</b> model (13B) with the structure of <b>diffusion transformer</b>.
</p>
<p><a href="#top">[back to top]</a></p>
<!-- End of a paper -->



<!-- Beginning of a paper -->
<h4 id="2024-07-19-t2vcompbench">
    [2024-07-19] T2V-CompBench: A Comprehensive Benchmark for Compositional Text-to-video Generation <i>(arXiv 2024)</i>
</h4>
<p>
    <b>Authors:</b> Kaiyue Sun, Kaiyi Huang, Xian Liu, Yue Wu, Zihan Xu, Zhenguo Li, Xihui Liu
</p>
<p>
    <b>Organizations:</b> The University of Hong Kong, The Chinese University of Hong Kon, Huawei Noah's Ark Lab
</p>
<p>
    <b>Summary:</b> Use 1400 prompts to evaluate <b>video generation</b> on <b>compositional generation</b>, including consistent attribute binding, dynamic attribute binding, sptial relationships, motion binding, action binding, object interations, generative numeracy.
</p>
<ul>
    <li> Find nouns and verbs by identifying them using WordNet from Pika Discord channels, used to generate prompts by GPT-4.
    <li> <u>Consistent attribute binding:</u> two objects, two attributes, and at least one active verb from color, shape, texture, and human-related attributes.
    <li> <u>Dynamic attribute binding:</u> color and light change, shape and size change, texture change, combined change.
    <li> <u>Spatial relationships:</u> two objects with spatial relationships like "on the left of".
    <li> <u>Motion binding:</u> one or two objects with specified moving direction like "leftwards".
    <li> <u>Action binding:</u> bind actions to corresponding objects.
    <li> <u>Object interactions:</u> dynamic interactions like pysical interactions.
    <li> <u>Generative numeracy:</u> a specific number of objects.
    <li> <u>Video LLM-based metrics (Grid-LLaVa)</u> is used for evaluating consistent attribute binding, action binding, object interactions.
    <li> <u>Image LLM-based metrics (LLaVa)</u> is used for evaluating dynamic attribute binding.
    <li> <u>Grounding DINO</u> is used for evaluating spatial relationships and numeracy.
    <li> <u>Grounding SAM + DOT</u> is used for evaluating motion binding.
</ul>
<table>
    <tr>
        <td>
            <figure>
                <img src='figs_reading_papers/2024-07-19-t2vcompbench-fig1.png' width=900>
                <figcaption>T2V-CompBench: categories (left), evaluation methods (middle), and benchmarking model performance (right).</figcaption>
            </figure>
        </td>
    </tr>
</table>
<p><a href="#top">[back to top]</a></p>
<!-- End of a paper -->



<!-- Beginning of a paper -->
<h4 id="2024-03-08-DPG-Bench">
    [2024-03-08] ELLA: Equip Diffusion Models with LLM for Enhanced Semantic Alignment
</h4>
<p>
    <b>Authors:</b> Xiwei Hu, Rui Wang, Yixiao Fang, Bin Fu, Pei Cheng, Gang Yu
</p>
<p>
    <b>Organizations:</b> Tencent
</p>
<p>
    <b>Summary:</b> ELLA: Replace CLIP with <b>LLM</b> to understand dense prompts; DPG-Bench: evaluate <b>image generation</b> on <b>dense prompts</b>.
</p>
<table>
    <tr>
        <td>
            <figure>
                <img src='figs_reading_papers/2024-03-08-DPG-Bench-fig1.png' width=800>
            </figure>
        </td>
    </tr>
</table>
<p><a href="#top">[back to top]</a></p>
<!-- End of a paper -->



<!-- Beginning of a paper -->
<h4 id="2023-12-19-InstructVideo">
    [2023-12-19] InstructVideo: Instructing Video Diffusion Models with Human Feedback <i>(CVPR 2024)</i>
</h4>
<p>
    <b>Authors:</b> Hangjie Yuan, Shiwei Zhang, Xiang Wang, Yujie Wei, Tao Feng, Yining Pan, Yingya Zhang, Ziwei Liu, Samuel Albanie, Dong Ni
</p>
<p>
    <b>Organizations:</b> Zhejiang University, Alibaba Group, Tsinghua University, Singapore University of Technology and Design, Nanyang Technological University, University of Cambridge
</p>
<p>
    <b>Summary:</b> Use HPS v2 to provide reward and train <b>video generation</b> models in an editing manner.
</p>
<ul>
    <li>
</ul>
<table>
    <tr>
        <td>
            <figure>
                <img src='figs_reading_papers/2023-12-19-InstructVideo-fig1.png' width=700>
                <figcaption>It takes reward fine-tuning as an <u>editing task</u> to accelerate training. The reward model is HPS v2. TAR assigns larger weight to central frames.</figcaption>
            </figure>
        </td>
    </tr>
</table>
<p><a href="#top">[back to top]</a></p>
<!-- End of a paper -->



<!-- Beginning of a paper -->
<h4 id="2023-11-29-vbench">
    [2023-11-29] VBench: Comprehensive Benchmark Suite for Video Generative Models
</h4>
<p>
    <b>Authors:</b> Ziqi Huang, Yinan He, Jiashuo Yu, Fan Zhang, Chenyang Si, Yuming Jiang, Yuanhan Zhang, Tianxing Wu, Qingyang Jin, Nattapol Chanpaisit, Yaohui Wang, Xinyuan Chen, Limin Wang, Dahua Lin, Yu Qiao, Ziwei Liu
</p>
<p>
    <b>Organizations:</b> Nanyang Technological University, Shanghai Artificial Intelligence Laboratory, The Chinese University of Hong Kong, Nanjing University
</p>
<p>
    <b>Summary:</b> Evaluate <b>video generation</b> from 16 dimensions within the perspectives of video quality and video-prompt consistency.
</p>
<ul>
    <li> <u>Content Categories:</u> <u>animal</u>, <u>architecture</u>, <u>food</u>, <u>human</u>, <u>lifestyle</u>, <u>plant</u>, <u>scenary</u>, <u>vehicles</u>.
    <li> <u>Temporal quality-subject consistency:</u> DINO feature similarity across frames.
    <li> <u>Temporal quality-background consistency:</u> CLIP feature similarity across frames.
    <li> <u>Temporal quality-temporal flickering:</u> mean absolute difference across frames.
    <li> <u>Temporal quality-motion smoothness:</u> use video frame interpolation model to evaluate motion smoothness.
    <li> <u>Temporal quality-dynamic degree:</u> use RAFT to estimate degree of dynamics.
    <li> <u>Frame-wise quality-aesthetic quality:</u> use LAION aesthetic predictor.
    <li> <u>Frame-wise quality-imaging quality:</u> use MUSIQ image quality predictor.
    <li> <u>Semantics-object class:</u> use GRiT to detect classes.
    <li> <u>Semantics-multiple objects:</u> detect success rate of generating all objects.
    <li> <u>Semantics-human action:</u> use UMT to detect specific actions.
    <li> <u>Semantics-color:</u> use GRiT for color captioning.
    <li> <u>Semantics-spatial relationship:</u> use rule-based evaluation.
    <li> <u>Semantics-scene:</u> use Tag2Text for scene captioning.
    <li> <u>Style-appearance style:</u> use CLIP feature similarity.
    <li> <u>Style-temporal style:</u> use ViCLIP to calculate video feature and temporal style description feature similarity.
    <li> <u>Overall consistency:</u> use ViCLIP to evaluate overall semantics and style consistency.
</ul>
<table>
    <tr>
        <td>
            <figure>
                <img src='figs_reading_papers/2023-11-29-vbench-fig1.png' width=900>
                <figcaption>caption1.</figcaption>
            </figure>
        </td>
    </tr>
</table>
<p><a href="#top">[back to top]</a></p>
<!-- End of a paper -->




<!-- Beginning of a paper -->
<h4 id="2023-10-17-GenEval">
    [2023-10-17] GenEval: An Object-Focused Framework for Evaluating Text-to-Image Alignment
</h4>
<p>
    <b>Authors:</b> Dhruba Ghosh, Hanna Hajishirzi, Ludwig Schmidt
</p>
<p>
    <b>Organizations:</b> University of Washington, Allen Institute for AI, LAION
</p>
<p>
    <b>Summary:</b> An <b>object-focused</b> framework for <b>image generation</b> evaluation by providing scores of single object, two objects, counting, colors, position, attribute binding, and overall.
</p>
<table>
    <tr>
        <td>
            <figure>
                <img src='figs_reading_papers/2023-10-17-GenEval-fig1.png' width=700>
                <figcaption>GenEval detects objects using Mask2Former detector and evaluates attributes of them.</figcaption>
            </figure>
        </td>
    </tr>
    <tr>
        <td>
            <figure>
                <img src='figs_reading_papers/2023-10-17-GenEval-fig2.png' width=600>
                <figcaption>Specific evaluation perspectives of GenEval.</figcaption>
            </figure>
        </td>
    </tr>
</table>
<p><a href="#top">[back to top]</a></p>
<!-- End of a paper -->



<!-- Beginning of a paper -->
<h4 id="2023-07-12-t2icompbench">
    [2023-07-12] T2I-CompBench: A Comprehensive Benchmark for Open-world Compositional Text-to-image Generation <i>(NeurIPS 2023)</i>
</h4>
<p>
    <b>Authors:</b> Kaiyi Huang, Kaiyue Sun, Enze Xie, Zhenguo Li, Xihui Liu
</p>
<p>
    <b>Organizations:</b> The University of Hong Kong, Huawei Noah's Ark Lab
</p>
<p>
    <b>Summary:</b> Use 6000 prompts to evaluate <b>image generation</b> on <b>compositional generation</b>, including attribute binding, object relationship, and complex compositions.
</p>
<ul>
    <li> <u>Attribute binding prompts:</u> at least two objects with two attributes from color, shape, texture.
    <li> <u>Object relationship prompts:</u> at least two objects with spatial relationship or non-spatial relationship.
    <li> <u>Complex compositions prompts:</u> more than two objects or more than two sub-categories.
    <li> Introduce Generative mOdel finetuning with Reward-driven Sample selection (GORS) that finetunes LoRA of SD2 with generated images which highly align with the compositional prompts.
</ul>
<table>
    <tr>
        <td>
            <figure>
                <img src='figs_reading_papers/2023-07-12-t2icompbench-fig1.png' width=900>
                <figcaption><li> Use <u>disentangled BLIP-VQA</u> to evaluate attribute binding, <u>UniDet-based mstric</u> to evaluate spatial relationship, <u>CLIPScore</u> to evaluate non-spatial relationship, <u>3-in-1 metric</u> (average score of the three metrics) to evaluate complex compositions.</figcaption>
            </figure>
        </td>
    </tr>
</table>
<p><a href="#top">[back to top]</a></p>
<!-- End of a paper -->



<!-- Beginning of a paper -->
<h4 id="2023-06-15-hpsv2">
    [2023-06-15] Human Preference Score v2: A Solid Benchmark for Evaluating Human Preferences of Text-to-Image Synthesis <i>(arXiv 2023)</i>
</h4>
<p>
    <b>Authors:</b> Xiaoshi Wu, Yiming Hao, Keqiang Sun, Yixiong Chen, Feng Zhu, Rui Zhao, Hongsheng Li
</p>
<p>
    <b>Organizations:</b> CUHK, SenseTime Research, Shanghai Jiao Tong University, Centre for Perceptual and Interactive Intelligence
</p>
<p>
    <b>Summary:</b> HPD v2: 798K binary human preference choices on 433K pairs of generated images; HPS v2: use HPD v2 to fine-tune CLIP for <b>image generation</b> evaluation.
</p>
<table>
    <tr>
        <td>
            <figure>
                <img src='figs_reading_papers/2023-06-15-hpsv2-fig1.png' width=900>
                <figcaption>Step 1: Clean prompts from COCO captions and DiffusionDB by ChatGPT; Step 2: Generate images using 9 models; Step 3: Rand and annotate each pair of images; Step 4: Train CLIP and obtain preference model for providing HPS v2 score.</figcaption>
            </figure>
        </td>
    </tr>
</table>
<p><a href="#top">[back to top]</a></p>
<!-- End of a paper -->



<!-- Beginning of a paper -->
<h4 id="2023-04-12-imagereward">
    [2023-04-12] ImageReward: Learning and Evaluating Human Preferences for Text-to-Image Generation <i>(NeurIPS 2023)</i>
</h4>
<p>
    <b>Authors:</b> Jiazheng Xu, Xiao Liu, Yuchen Wu, Yuxuan Tong, Qinkai Li, Ming Ding, Jie Tang, Yuxiao Dong
</p>
<p>
    <b>Organizations:</b> Tsinghua University, Zhipu AI, Beijing University of Posts and Telecommunications
</p>
<p>
    <b>Summary:</b> Train BLIP on 137K human preference image pairs for <b>image generation</b> and use it to tune diffusion models by Reward Feedback Learning (ReFL).
</p>
<ul>
    <li>
</ul>
<table>
    <tr>
        <td>
            <figure>
                <img src='figs_reading_papers/2023-04-12-imagereward-fig1.png' width=700>
                <figcaption>Step 1: use DiffusionDB prompts to generate images; Step 2: Rate and rank images; Step 3: Train ImageReward using ranking data; Step 4: tune diffusion models using ImageReward; Step 5: provide scores to the generated images.</figcaption>
            </figure>
        </td>
    </tr>
</table>
<p><a href="#top">[back to top]</a></p>
<!-- End of a paper -->



<!-- Beginning of a paper -->
<h4 id="2023-03-25-hps">
    [2023-03-25-hps] Human Preference Score: Better Aligning Text-to-Image Models with Human Preference <i>(ICCV 2023)</i>
</h4>
<p>
    <b>Authors:</b> Xiaoshi Wu, Keqiang Sun, Feng Zhu, Rui Zhao, Hongsheng Li
</p>
<p>
    <b>Organizations:</b> CUHK, SenseTime Research, Shanghai Jiao Tong University, Centre for Perceptual and Interactive Intelligence, Shanghai AI Lab
</p>
<p>
    <b>Summary:</b> Fine-tune CLIP using annotated 98K SD generated images from 25K prompts for <b>image generation</b> evaluation.
</p>
<ul>
    <li> Both IS and FID are not aligned well with human preference.
    <li> The calculation of HPS is similar to CLIPScore, except for multiplying by 100 rather than 2.5.
</ul>
<table>
    <tr>
        <td>
            <figure>
                <img src='figs_reading_papers/2023-03-25-hps-fig1.png' width=900>
                <figcaption>Train human preference classifier: similar to CLIP except for the sample with the highest preference is taken as the positive; append a special token to the prompts of worse images and train SD LoRA; remove the special token during inference.</figcaption>
            </figure>
        </td>
    </tr>
</table>
<p><a href="#top">[back to top]</a></p>
<!-- End of a paper -->




<!-- Beginning of a paper -->
<h4 id="2022-12-19-DiT">
    [2022-12-19] Scalable Diffusion Models with Transformers <i>(ICCV 2023)</i>
</h4>
<p>
    <b>Authors:</b> William Peebles, Saining Xie
</p>
<p>
    <b>Organizations:</b> UC Berkeley, New York University
</p>
<p>
    <b>Summary:</b> Replace U-Net by <b>transformer</b> for scalable <b>image generation</b>.
</p>
<ul>
    <li> <u>Increase model size</u> and <u>decrease patch size</u> yield considerably improved diffusion models.
    <li> <u>Scale model Gflops</u> is the key to improved performance.
    <li> <u>Larger DiT models</u> are more <u>compute-efficient</u>.
</ul>
<table>
    <tr>
        <td>
            <figure>
                <img src='figs_reading_papers/2022-12-19-DiT-fig1.png' width=800>
                <figcaption>Use <u>adaLN-Zero</u> structure to inject timestep and prompt is better than cross-attention or in-context.</figcaption>
            </figure>
        </td>
    </tr>
</table>
<p><a href="#top">[back to top]</a></p>
<!-- End of a paper -->




<!-- Beginning of a paper -->
<h4 id="2022-12-19-promptist">
    [2022-12-19] Optimizing Prompts for Text-to-Image Generation <i>(NeurIPS 2023)</i>
</h4>
<p>
    <b>Authors:</b> Yaru Hao, Zewen Chi, Li Dong, Furu Wei
</p>
<p>
    <b>Organizations:</b> Microsoft Research
</p>
<p>
    <b>Summary:</b> Use language models for <b>image generation</b> <b>prompt refinement</b> by taking relevance and aesthetics as reward.
</p>
<table>
    <tr>
        <td>
            <figure>
                <img src='figs_reading_papers/2022-12-19-promptist-fig1.png' width=600>
                <figcaption>Step 1: fine-tune a language model (LM) to optimize prompts; Step 2: further fine-tune LM with PPO, where aesthetic and relevance are the reward.</figcaption>
            </figure>
        </td>
    </tr>
</table>
<p><a href="#top">[back to top]</a></p>
<!-- End of a paper -->



<!-- Beginning of a paper -->
<h4 id="2022-08-25-Unified-Perspective">
    [2022-08-25] Understanding Diffusion Models: A Unified Perspective <i>(arXiv 2022)</i>
</h4>
<p>
    <b>Authors:</b> Calvin Luo
</p>
<p>
    <b>Organizations:</b> Google Brain
</p>
<p>
    <b>Summary:</b> Introduction to <u>VAE</u>, <u>DDPM</u>, <u>score-based generative model</u>, <u>guidance</u> from a <b>unified generative perspective<b>.
</p>
<p><a href="#top">[back to top]</a></p>
<!-- End of a paper -->



<!-- Beginning of a paper -->
<h4 id="2022-05-29-cogvideo">
    [2022-05-29] CogVideo: Large-scale Pretraining for Text-to-Video Generation via Transformers <i>(ICLR 2023)</i>
</h4>
<p>
    <b>Authors:</b> Wenyi Hong, Ming Ding, Wendi Zheng, Xinghan Liu, Jie Tang
</p>
<p>
    <b>Organizations:</b> Tsinghua University, BAAI
</p>
<p>
    <b>Summary:</b> An <u>open-sourced</u> <b>transformer</b>-based <b>video generation</b> model (9B) that <b>auto-regressively</b> generates frame sequences and then performs frame interpolatation.
</p>
<table>
    <tr>
        <td>
            <figure>
                <img src='figs_reading_papers/2022-05-29-cogvideo-fig1.png' width=500>
                <figcaption>It is trained on CogView2. In stage 1, it generates frames sequentially. In stage 2, it recursively interpolates frames.</figcaption>
            </figure>
        </td>
    </tr>
</table>
<p><a href="#top">[back to top]</a></p>
<!-- End of a paper -->



<!-- Beginning of a paper -->
<h4 id="2021-12-20-ldm">
    [2021-12-20] High-Resolution Image Synthesis with Latent Diffusion Models <i>(CVPR 2022)</i>
</h4>
<p>
    <b>Authors:</b> Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, Björn Ommer
</p>
<p>
    <b>Organizations:</b> Heidelberg University, Runway ML
</p>
<p>
    <b>Summary:</b> Efficient high-quality <b>image generation</b> by applying <b>U-Net diffusion models</b> in the <b>VAE latent space</b>.
</p>
<table>
    <tr>
        <td>
            <figure>
                <img src='figs_reading_papers/2021-12-20-ldm-fig1.png' width=400>
                <figcaption>LDM performs the diffusion process in the latent space of VAE.</figcaption>
            </figure>
        </td>
    </tr>
</table>
<p><a href="#top">[back to top]</a></p>
<!-- End of a paper -->
 

<!-- Beginning of a paper -->
<h4 id="2021-04-18-clipscore">
    [2021-04-18] CLIPScore: A Reference-free Evaluation Metric for Image Captioning <i>(EMNLP 2021)</i>
</h4>
<p>
    <b>Authors:</b> Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras, Yejin Choi
</p>
<p>
    <b>Organizations:</b> Allen Institute for AI, University of Washington
</p>
<p>
    <b>Summary:</b> A reference-free metric mainly focusing on <b>semantic alignment</b> for <b>image generation</b> evaluation.
</p>
<ul>
    <li> CLIPScore calculates <u>the cosine similarity between a caption and an image, multiplying the result by 2.5</u>.
    <li> CLIPScore is  <u>complementary to common metrics</u> (like BLEU-4, SPICE, and CIDEr).
    <li> "A photo depicts" as a prompt to CLIP is sligtly better than the official "A photo of".
    <li> CLIPScore is sensitive to adversarially constructed image captions.
    <li> CLIPScore can reconstruct judgments on never-before-seen images.
    <li> Propose a reference-augmented version of CLIPScore, called RefCLIPScore, which is a harmonic mean of CLIPScore and the maximum of cosine similarity between the caption and all the references.
</ul>
<table>
    <tr>
        <td>
            <figure>
                <img src='figs_reading_papers/2021-04-18-clipscore-fig1.png' width=400>
                <figcaption>CLIPScore frees from the shortcomings of n-gram matching that disfavors good captions with new words and favors captions with familiar words.</figcaption>
            </figure>
        </td>
    </tr>
</table>
<p><a href="#top">[back to top]</a></p>
<!-- End of a paper -->


<!-- Beginning of a paper -->
<h4 id="2020-12-16-ddpm">
    [2020-12-16] Denoising Diffusion Probabilistic Models <i>(NeurIPS 2020)</i>
</h4>
<p>
    <b>Authors:</b> Jonathan Ho, Ajay Jain, Pieter Abbeel
</p>
<p>
    <b>Organizations:</b> UC Berkeley
</p>
<p>
    <b>Summary:</b> Denoising <b>diffusion</b> probabilistic models that iteratively denoises data from random noise for <b>image generation</b>.
</p>
<table>
    <tr>
        <td>
            <figure>
                <img src='figs_reading_papers/2020-12-16-ddpm-fig1.png' width=400>
                <figcaption>Forward and reverse processes of DDPM.</figcaption>
            </figure>
        </td>
        <td>
            <figure>
                <img src='figs_reading_papers/2020-12-16-ddpm-fig2.png' width=500>
        <figcaption>Training and sampling algorithms of DDPM.</figcaption>
            </figure>
        </td>
    </tr>
</table>
<p><a href="#top">[back to top]</a></p>
<!-- End of a paper -->



<!-- Beginning of a paper -->
<h4 id="2019-05-04-fvd">
    [2019-05-04] FVD: A new Metric for Video Generation <i>(ICLR workshop 2019)</i>
</h4>
<p>
    <b>Authors:</b> Thomas Unterthiner, Sjoerd van Steenkiste, Karol Kurach, Raphaël Marinier, Marcin Michalski, Sylvain Gelly
</p>
<p>
    <b>Organizations:</b> Johannes Kepler University, IDSIA, Google Brain
</p>
<p>
    <b>Summary:</b> Extend FID for <b>video generation</b> evaluation by replacing 2D InceptionNet with pre-trained Inflated 3D convnet.
</p>

<p><a href="#top">[back to top]</a></p>
<!-- End of a paper -->



<!-- Beginning of a paper -->
<h4 id="2017-06-26-fid">
    [2017-06-26] GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium <i>(NeurIPS 2017)</i>
</h4>
<p>
    <b>Authors:</b> Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, Sepp Hochreiter
</p>
<p>
    <b>Organizations:</b> Johannes Kepler University Linz
</p>
<p>
    <b>Summary:</b> Calculate <b>Fréchet distance</b> between Gaussian distributions of InceptionNet feature maps of real-world data and synthetic data for <b>image generation</b> evaluation.
</p>

<p><a href="#top">[back to top]</a></p>
<!-- End of a paper -->



<!-- Beginning of a paper -->
<h4 id="2016-06-10-inception-score">
    [2016-06-10] Improved Techniques for Training GANs <i>(NeurIPS 2016)</i>
</h4>
<p>
    <b>Authors:</b> Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, Xi Chen
</p>
<p>
    <b>Organizations:</b> OpenAI
</p>
<p>
    <b>Summary:</b> Calculate <b>KL divergence between p(y|x) and p(y)</b> that aims to minimize the entropy across samples and maximize the entropy across classes for <b>image generation</b> evaluation.
</p>
</table>
<p><a href="#top">[back to top]</a></p>
<!-- End of a paper -->

</body>
</html>