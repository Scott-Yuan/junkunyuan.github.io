{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "807b8515-0ea0-46f0-b8ce-6ca40a2dbf4a",
   "metadata": {},
   "source": [
    "# T2I-CompBench\n",
    "\n",
    "Introduction to T2I-CompBench used for image generation evaluation.\n",
    "\n",
    "Written by yuanjk0921@outlook.com\n",
    "\n",
    "See more reading papers and notes [here](https://junkunyuan.github.io/reading_papers/reading_papers.html)\n",
    "\n",
    "Updated on Feb 28, 2025"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcbdc0c2-5aa1-4b6e-b446-04cc3c8a0b85",
   "metadata": {},
   "source": [
    "**References**\n",
    "- [**T2I-CompBench: A Comprehensive Benchmark for Open-world Compositional Text-to-image Generation** *(NeurIPS 2023)*](https://arxiv.org/pdf/2307.06350): The paper to introduce the T2I-CompBench."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efb390bf-f621-4d4b-b2cb-b6b70e682640",
   "metadata": {},
   "source": [
    "T2I-CompBench is designed for **image generation** on <u>compositional generation</u>, including attribute binding, object relationship, and complex compositions.\n",
    "\n",
    "## 1. Attribute Binding\n",
    "\n",
    "### 1.1 Color\n",
    "\n",
    "**Data:** [All data (1000 prompts)](https://github.com/Karine-Huang/T2I-CompBench/blob/main/examples/dataset/color.txt) & [Training data (700 prompts)](https://github.com/Karine-Huang/T2I-CompBench/blob/main/examples/dataset/color_train.txt) & [Test data (300 prompts)](https://github.com/Karine-Huang/T2I-CompBench/blob/main/examples/dataset/color_val.txt)\n",
    "\n",
    "**Metrics:** use the VQA ability of BLIP for evaluating the probability of answering \"yes\".\n",
    "\n",
    "**Sources:** 480 prompts from CC500, 200 prompts from COCO, and 320 prompts generated by ChatGPT.\n",
    "\n",
    "**Example:** \"a green bench and a blue bowl\"\n",
    "\n",
    "### 1.2 Shape\n",
    "\n",
    "[All data (1000 prompts)](https://github.com/Karine-Huang/T2I-CompBench/blob/main/examples/dataset/shape.txt) & [Training data (700 prompts)](https://github.com/Karine-Huang/T2I-CompBench/blob/main/examples/dataset/shape_train.txt) & [Test data (300 prompts)](https://github.com/Karine-Huang/T2I-CompBench/blob/main/examples/dataset/shape_val.txt)\n",
    "\n",
    "**Metrics:** use the VQA ability of BLIP for evaluating the probability of answering \"yes\".</u>\n",
    "\n",
    "**Sources:** generated by ChatGPT by prompting with the shape set of {long, tall, short, big, small, cubic, ...}.\n",
    "\n",
    "**Example:** \"a pyramidal paperweight and a teardrop pen\"\n",
    "\n",
    "### 1.3 Texture\n",
    "\n",
    "[All data (1000 prompts)](https://github.com/Karine-Huang/T2I-CompBench/blob/main/examples/dataset/texture.txt) & [Training data (700 prompts)](https://github.com/Karine-Huang/T2I-CompBench/blob/main/examples/dataset/texture_train.txt) & [Test data (300 prompts)](https://github.com/Karine-Huang/T2I-CompBench/blob/main/examples/dataset/texture_val.txt)\n",
    "\n",
    "**Metrics:** use the VQA ability of BLIP for evaluating the probability of answering \"yes\".\n",
    "\n",
    "**Sources:** 800 prompts generated by random combinations of texture sets and 200 prompts are generated by ChatGPT.\n",
    "\n",
    "**Example:** \"a rubber eraser and a metallic key\"\n",
    "\n",
    "## 2. Object Relationship\n",
    "\n",
    "### 2.1 Spatial relationships\n",
    "\n",
    "[All data (1000 prompts)](https://github.com/Karine-Huang/T2I-CompBench/blob/main/examples/dataset/spatial.txt) & [Training data (700 prompts)](https://github.com/Karine-Huang/T2I-CompBench/blob/main/examples/dataset/spatial_train.txt) & [Test data (300 prompts)](https://github.com/Karine-Huang/T2I-CompBench/blob/main/examples/dataset/spatial_val.txt)\n",
    "\n",
    "**Metrics:** use UniDet to detect objects and detetmine the spatial relationship by comparing the bounding boxes.\n",
    "\n",
    "**Sources:** random combinations of a spatial set of {on the side of, next to, on the right of, ...}\n",
    "\n",
    "**Example:** \"a book on the top of a woman\"\n",
    "\n",
    "\n",
    "### 2.2 Non-spatial relationships\n",
    "\n",
    "[All data (1000 prompts)](https://github.com/Karine-Huang/T2I-CompBench/blob/main/examples/dataset/non_spatial.txt) & [Training data (700 prompts)](https://github.com/Karine-Huang/T2I-CompBench/blob/main/examples/dataset/non_spatial_train.txt) & [Test data (300 prompts)](https://github.com/Karine-Huang/T2I-CompBench/blob/main/examples/dataset/non_spatial_val.txt)\n",
    "\n",
    "**Metrics:** CLIPScore.\n",
    "\n",
    "**Sources:** generated by ChatGPT.\n",
    "\n",
    "**Examples:** \"A person is yawning in a boring meeting.\"\n",
    "\n",
    "## 3. Complex Compositions\n",
    "\n",
    "[All data (1000 prompts)](https://github.com/Karine-Huang/T2I-CompBench/blob/main/examples/dataset/complex.txt) & [Training data (700 prompts)](https://github.com/Karine-Huang/T2I-CompBench/blob/main/examples/dataset/complex_train.txt) & [Test data (300 prompts)](https://github.com/Karine-Huang/T2I-CompBench/blob/main/examples/dataset/complex_val.txt)\n",
    "\n",
    "**Metrics:** the average score of each previous dimension.\n",
    "\n",
    "**Sources:** generate 250 prompts with ChatGPT for each of the four scenarios: \"two objects with multiple attributes\", \"two objects with mixed attributes\", \"more than two objects with multiple attributes\", \"more than two objects with mixed attributes\".\n",
    "\n",
    "**Example:** \"The soft blanket draped over the bumpy couch and the hard floor.\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
